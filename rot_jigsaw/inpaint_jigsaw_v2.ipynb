{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torchvision.models import resnet18\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from inpainting import Encoder, Decoder, InpaintingModel, Discriminator, mask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device (cuda or cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawResNet(nn.Module):\n",
    "    def __init__(self, num_patches, num_permutations, backbone=None):\n",
    "        super(JigsawResNet, self).__init__()\n",
    "        if backbone is not None:\n",
    "            self.backbone = backbone  # Use the encoder from the inpainting model\n",
    "            print(\"Using pretrained backbone from inpainting model\")\n",
    "        else:\n",
    "            resnet = models.resnet18(weights=None)\n",
    "            self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "            print(\"Using new backbone\")\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.num_permutations = num_permutations\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * num_patches, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_permutations)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_patches, channels, height, width = x.shape\n",
    "        patches = []\n",
    "        for i in range(num_patches):\n",
    "            patch = x[:, i]\n",
    "            features = self.backbone(patch)\n",
    "            features = torch.flatten(features, start_dim=1)\n",
    "            patches.append(features)\n",
    "        concatenated_features = torch.cat(patches, dim=1)\n",
    "        output = self.fc(concatenated_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using new encoder\n"
     ]
    }
   ],
   "source": [
    "# Initialize the inpainting model\n",
    "inpainting_model = InpaintingModel().to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "inpainting_checkpoint_path = 'inpainting_model_gen_weights_epoch_100.pth'\n",
    "inpainting_checkpoint = torch.load(inpainting_checkpoint_path, map_location=device,weights_only=False )\n",
    "inpainting_model.load_state_dict(inpainting_checkpoint)\n",
    "\n",
    "# Extract the encoder from the inpainting model\n",
    "inpainting_encoder = inpainting_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# STL-10 Dataset\n",
    "train_dataset = datasets.STL10(root='./data', split='train+unlabeled', download=True, transform=transform)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Define Jigsaw Dataset\n",
    "class JigsawSTL10Dataset(Dataset):\n",
    "    def __init__(self, dataset, grid_size=3):\n",
    "        self.dataset = dataset\n",
    "        self.grid_size = grid_size\n",
    "        self.permutations = self.create_permutations()\n",
    "        \n",
    "    def create_permutations(self, num_patches=9, num_permutations=1000):\n",
    "        all_permutations = list(itertools.permutations(range(num_patches)))\n",
    "        random.seed(42)\n",
    "        selected_permutations = random.sample(all_permutations, num_permutations)\n",
    "        return selected_permutations\n",
    "    \n",
    "    def create_jigsaw_puzzle(self, image):\n",
    "        _, height, width = image.shape\n",
    "        grid_size = self.grid_size\n",
    "        patch_h, patch_w = height // grid_size, width // grid_size\n",
    "        patches = []\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                patch = image[:, i * patch_h: (i + 1) * patch_h, j * patch_w: (j + 1) * patch_w]\n",
    "                patches.append(patch)\n",
    "        perm_class = random.choice(range(len(self.permutations)))\n",
    "        perm = self.permutations[perm_class]\n",
    "        shuffled_patches = [patches[i] for i in perm]\n",
    "        return torch.stack(shuffled_patches), torch.tensor(perm_class, dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.dataset[index]\n",
    "        shuffled_patches, perm_class = self.create_jigsaw_puzzle(img)\n",
    "        return shuffled_patches, perm_class\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Create Jigsaw datasets\n",
    "jigsaw_train_dataset = JigsawSTL10Dataset(train_dataset)\n",
    "jigsaw_val_dataset = JigsawSTL10Dataset(val_dataset)\n",
    "\n",
    "# DataLoaders\n",
    "jigsaw_train_loader = DataLoader(jigsaw_train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "jigsaw_val_loader = DataLoader(jigsaw_val_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained backbone from inpainting model\n",
      "Epoch [1/10], Training Loss: 6.7007\n",
      "Epoch [1/10], Validation Loss: 7.2744\n",
      "Model saved at Epoch 1\n",
      "Epoch [2/10], Training Loss: 5.5953\n",
      "Epoch [2/10], Validation Loss: 5.4044\n",
      "Model saved at Epoch 2\n",
      "Epoch [3/10], Training Loss: 4.7806\n",
      "Epoch [3/10], Validation Loss: 5.0467\n",
      "Model saved at Epoch 3\n",
      "Epoch [4/10], Training Loss: 4.3170\n",
      "Epoch [4/10], Validation Loss: 4.0588\n",
      "Model saved at Epoch 4\n",
      "Epoch [5/10], Training Loss: 4.0063\n",
      "Epoch [5/10], Validation Loss: 3.7886\n",
      "Model saved at Epoch 5\n",
      "Epoch [6/10], Training Loss: 3.8144\n",
      "Epoch [6/10], Validation Loss: 3.6389\n",
      "Model saved at Epoch 6\n",
      "Epoch [7/10], Training Loss: 3.6655\n",
      "Epoch [7/10], Validation Loss: 3.5072\n",
      "Model saved at Epoch 7\n",
      "Epoch [8/10], Training Loss: 3.5444\n",
      "Epoch [8/10], Validation Loss: 3.4443\n",
      "Model saved at Epoch 8\n",
      "Epoch [9/10], Training Loss: 3.4310\n",
      "Epoch [9/10], Validation Loss: 3.6864\n",
      "Epoch [10/10], Training Loss: 3.3321\n",
      "Epoch [10/10], Validation Loss: 3.7105\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = '/users/nladdha/my_env/model/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Initialize the Jigsaw model with the encoder from the inpainting model\n",
    "num_permutations = 1000\n",
    "num_patches = 9\n",
    "jigsaw_model = JigsawResNet(num_patches=num_patches, num_permutations=num_permutations, backbone=inpainting_encoder).to(device)\n",
    "\n",
    "# Ensure the backbone is unfrozen for training\n",
    "for param in jigsaw_model.backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(jigsaw_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    jigsaw_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for shuffled_patches, perm_class in jigsaw_train_loader:\n",
    "        shuffled_patches, perm_class = shuffled_patches.to(device), perm_class.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = jigsaw_model(shuffled_patches)\n",
    "        loss = criterion(outputs, perm_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(jigsaw_train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    jigsaw_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_patches, val_perm_class in jigsaw_val_loader:\n",
    "            val_patches, val_perm_class = val_patches.to(device), val_perm_class.to(device)\n",
    "            val_outputs = jigsaw_model(val_patches)\n",
    "            val_loss += criterion(val_outputs, val_perm_class).item()\n",
    "    avg_val_loss = val_loss / len(jigsaw_val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Checkpointing\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model_path = os.path.join(checkpoint_dir, 'inpaint_jigsaw_model_testing.pth')\n",
    "        torch.save(jigsaw_model.state_dict(), model_path)\n",
    "        print(f\"Model saved at Epoch {epoch+1}\")\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    if no_improve_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping due to no improvement in validation loss.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new backbone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3810194/2681421820.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/users/nladdha/my_env/model/inpaint_jigsaw_model.pth', map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Initialize the Jigsaw model (same architecture as during training)\n",
    "combined_model = JigsawResNet(num_patches=num_patches, num_permutations=num_permutations, backbone=None).to(device)\n",
    "# Wrap the model\n",
    "combined_model = nn.DataParallel(combined_model)\n",
    "\n",
    "# Load the saved state_dict\n",
    "checkpoint = torch.load('/users/nladdha/my_env/model/inpaint_jigsaw_model.pth', map_location=device)\n",
    "state_dict = checkpoint\n",
    "\n",
    "# Adjust the keys in the state_dict\n",
    "adjusted_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # Remove 'encoder.' from keys that start with 'module.backbone.encoder.'\n",
    "    if key.startswith('module.backbone.encoder.'):\n",
    "        new_key = key.replace('encoder.', '')  # Remove 'encoder.' from the key\n",
    "        adjusted_state_dict[new_key] = value\n",
    "    else:\n",
    "        adjusted_state_dict[key] = value  # Keep other keys unchanged\n",
    "\n",
    "# Load the adjusted state_dict into the model\n",
    "combined_model.load_state_dict(adjusted_state_dict)\n",
    "\n",
    "# Since the model is wrapped with DataParallel, access the module to get the backbone\n",
    "combined_backbone = combined_model.module.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=10):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.encoder = encoder  # Combined encoder\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "classification_model = ClassificationModel(encoder=combined_backbone, num_classes=10).to(device)\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    classification_model = nn.DataParallel(classification_model)\n",
    "    \n",
    "# Optionally freeze the encoder\n",
    "for param in classification_model.encoder.parameters():\n",
    "    param.requires_grad = True  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the STL-10 training dataset\n",
    "full_train_dataset = torchvision.datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
    "\n",
    "# Split the training dataset into train and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = datasets.STL10(root='./data', split='test', download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 1.8735\n",
      "Epoch [1/150], Validation Loss: 1.6125\n",
      "Model saved at Epoch 1\n",
      "Epoch [2/150], Train Loss: 1.5183\n",
      "Epoch [2/150], Validation Loss: 1.4248\n",
      "Model saved at Epoch 2\n",
      "Epoch [3/150], Train Loss: 1.3550\n",
      "Epoch [3/150], Validation Loss: 1.2626\n",
      "Model saved at Epoch 3\n",
      "Epoch [4/150], Train Loss: 1.1815\n",
      "Epoch [4/150], Validation Loss: 1.2608\n",
      "Model saved at Epoch 4\n",
      "Epoch [5/150], Train Loss: 1.0828\n",
      "Epoch [5/150], Validation Loss: 1.1562\n",
      "Model saved at Epoch 5\n",
      "Epoch [6/150], Train Loss: 0.9924\n",
      "Epoch [6/150], Validation Loss: 1.1294\n",
      "Model saved at Epoch 6\n",
      "Epoch [7/150], Train Loss: 0.9322\n",
      "Epoch [7/150], Validation Loss: 1.0382\n",
      "Model saved at Epoch 7\n",
      "Epoch [8/150], Train Loss: 0.8469\n",
      "Epoch [8/150], Validation Loss: 1.1035\n",
      "Epoch [9/150], Train Loss: 0.7955\n",
      "Epoch [9/150], Validation Loss: 1.1346\n",
      "Epoch [10/150], Train Loss: 0.7258\n",
      "Epoch [10/150], Validation Loss: 1.0857\n",
      "Epoch [11/150], Train Loss: 0.6733\n",
      "Epoch [11/150], Validation Loss: 1.1291\n",
      "Epoch [12/150], Train Loss: 0.6532\n",
      "Epoch [12/150], Validation Loss: 1.0049\n",
      "Model saved at Epoch 12\n",
      "Epoch [13/150], Train Loss: 0.5997\n",
      "Epoch [13/150], Validation Loss: 1.0910\n",
      "Epoch [14/150], Train Loss: 0.5595\n",
      "Epoch [14/150], Validation Loss: 1.0559\n",
      "Epoch [15/150], Train Loss: 0.5216\n",
      "Epoch [15/150], Validation Loss: 1.0529\n",
      "Epoch [16/150], Train Loss: 0.4862\n",
      "Epoch [16/150], Validation Loss: 1.0762\n",
      "Epoch [17/150], Train Loss: 0.4590\n",
      "Epoch [17/150], Validation Loss: 1.0937\n",
      "Epoch [18/150], Train Loss: 0.3783\n",
      "Epoch [18/150], Validation Loss: 1.2034\n",
      "Epoch [19/150], Train Loss: 0.3683\n",
      "Epoch [19/150], Validation Loss: 1.3548\n",
      "Epoch [20/150], Train Loss: 0.3705\n",
      "Epoch [20/150], Validation Loss: 1.1873\n",
      "Epoch [21/150], Train Loss: 0.3247\n",
      "Epoch [21/150], Validation Loss: 1.3487\n",
      "Epoch [22/150], Train Loss: 0.2935\n",
      "Epoch [22/150], Validation Loss: 1.3533\n",
      "Early stopping\n",
      "Top-1 Accuracy: 65.90%\n",
      "Top-3 Accuracy: 89.10%\n",
      "Top-5 Accuracy: 96.74%\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "epochs = 150\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# Training Loop for the downstream task\n",
    "for epoch in range(epochs):\n",
    "    classification_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classification_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation Loop for early stopping\n",
    "    classification_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "            val_outputs = classification_model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_targets).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Model checkpointing based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model_save_path = '/users/nladdha/my_env/model/stl_inpaint_jigsaw_test.pth'\n",
    "        # Save the model, adjusting for DataParallel\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            torch.save(classification_model.module.state_dict(), model_save_path)\n",
    "        else:\n",
    "            torch.save(classification_model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved at Epoch {epoch+1}\")\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improve_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Evaluation with Top-1, Top-3, and Top-5 Accuracy\n",
    "classification_model.eval()\n",
    "correct = 0\n",
    "top_3_correct = 0\n",
    "top_5_correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = outputs.topk(5, dim=1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted[:, 0] == labels).sum().item()\n",
    "\n",
    "        # Top-3 accuracy\n",
    "        top_3_correct += (predicted[:, :3] == labels.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "        # Top-5 accuracy\n",
    "        top_5_correct += (predicted == labels.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top_3_accuracy = 100 * top_3_correct / total\n",
    "top_5_accuracy = 100 * top_5_correct / total\n",
    "\n",
    "print(f'Top-1 Accuracy: {accuracy:.2f}%')\n",
    "print(f'Top-3 Accuracy: {top_3_accuracy:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_5_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
