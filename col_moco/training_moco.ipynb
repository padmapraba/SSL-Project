{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models import ResNetGenerator\n",
    "from lightly.models.modules.heads import MoCoProjectionHead\n",
    "from lightly.models.utils import (\n",
    "    batch_shuffle,\n",
    "    batch_unshuffle,\n",
    "    deactivate_requires_grad,\n",
    "    update_momentum,\n",
    ")\n",
    "from lightly.transforms import MoCoV2Transform, utils\n",
    "\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models.modules import MoCoProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.moco_transform import MoCoV2Transform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 256  # STL-10 is larger, so a smaller batch size may be better for memory\n",
    "memory_bank_size = 4096\n",
    "seed = 1\n",
    "max_epochs = 100\n",
    "\n",
    "path_to_data = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.datasets import STL10\n",
    "\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models.modules import MoCoProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.moco_transform import MoCoV2Transform\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "\n",
    "\n",
    "class MoCo(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = MoCoProjectionHead(512, 512, 128)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        deactivate_requires_grad(self.backbone_momentum)\n",
    "        deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.backbone(x).flatten(start_dim=1)\n",
    "        query = self.projection_head(query)\n",
    "        return query\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        key = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        key = self.projection_head_momentum(key).detach()\n",
    "        return key\n",
    "\n",
    "\n",
    "resnet = torchvision.models.resnet18()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = MoCo(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "transform = MoCoV2Transform(input_size=96)\n",
    "dataset = torchvision.datasets.STL10(\n",
    "    root='../data', split='train+unlabeled', download=True, transform=transform\n",
    ")\n",
    "\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\", transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 00, loss: 7.89435\n",
      "epoch: 01, loss: 7.44988\n",
      "epoch: 02, loss: 7.29025\n",
      "epoch: 03, loss: 7.20489\n",
      "epoch: 04, loss: 7.15354\n",
      "epoch: 05, loss: 7.11737\n",
      "epoch: 06, loss: 7.09379\n",
      "epoch: 07, loss: 7.07331\n",
      "epoch: 08, loss: 7.05743\n",
      "epoch: 09, loss: 7.04059\n",
      "epoch: 10, loss: 7.02780\n",
      "epoch: 11, loss: 7.01487\n",
      "epoch: 12, loss: 7.00247\n",
      "epoch: 13, loss: 6.99387\n",
      "epoch: 14, loss: 6.98306\n",
      "epoch: 15, loss: 6.97650\n",
      "epoch: 16, loss: 6.96730\n",
      "epoch: 17, loss: 6.95951\n",
      "epoch: 18, loss: 6.95395\n",
      "epoch: 19, loss: 6.94624\n",
      "epoch: 20, loss: 6.93977\n",
      "epoch: 21, loss: 6.93605\n",
      "epoch: 22, loss: 6.92766\n",
      "epoch: 23, loss: 6.92371\n",
      "epoch: 24, loss: 6.91980\n",
      "epoch: 25, loss: 6.91480\n",
      "epoch: 26, loss: 6.91063\n",
      "epoch: 27, loss: 6.90630\n",
      "epoch: 28, loss: 6.90125\n",
      "epoch: 29, loss: 6.89666\n",
      "epoch: 30, loss: 6.89313\n",
      "epoch: 31, loss: 6.89133\n",
      "epoch: 32, loss: 6.88518\n",
      "epoch: 33, loss: 6.88483\n",
      "epoch: 34, loss: 6.88132\n",
      "epoch: 35, loss: 6.87815\n",
      "epoch: 36, loss: 6.87500\n",
      "epoch: 37, loss: 6.87133\n",
      "epoch: 38, loss: 6.86969\n",
      "epoch: 39, loss: 6.86777\n",
      "epoch: 40, loss: 6.86376\n",
      "epoch: 41, loss: 6.86293\n",
      "epoch: 42, loss: 6.86044\n",
      "epoch: 43, loss: 6.85706\n",
      "epoch: 44, loss: 6.85636\n",
      "epoch: 45, loss: 6.85252\n",
      "epoch: 46, loss: 6.85247\n",
      "epoch: 47, loss: 6.84884\n",
      "epoch: 48, loss: 6.84678\n",
      "epoch: 49, loss: 6.84442\n",
      "epoch: 50, loss: 6.84257\n",
      "epoch: 51, loss: 6.83983\n",
      "epoch: 52, loss: 6.84019\n",
      "epoch: 53, loss: 6.83956\n",
      "epoch: 54, loss: 6.83624\n",
      "epoch: 55, loss: 6.83396\n",
      "epoch: 56, loss: 6.83187\n",
      "epoch: 57, loss: 6.83061\n",
      "epoch: 58, loss: 6.83035\n",
      "epoch: 59, loss: 6.82833\n",
      "epoch: 60, loss: 6.82635\n",
      "epoch: 61, loss: 6.82556\n",
      "epoch: 62, loss: 6.82362\n",
      "epoch: 63, loss: 6.82237\n",
      "epoch: 64, loss: 6.82071\n",
      "epoch: 65, loss: 6.81967\n",
      "epoch: 66, loss: 6.81736\n",
      "epoch: 67, loss: 6.81698\n",
      "epoch: 68, loss: 6.81556\n",
      "epoch: 69, loss: 6.81366\n",
      "epoch: 70, loss: 6.81181\n",
      "epoch: 71, loss: 6.80985\n",
      "epoch: 72, loss: 6.81114\n",
      "epoch: 73, loss: 6.80977\n",
      "epoch: 74, loss: 6.80798\n",
      "epoch: 75, loss: 6.80676\n",
      "epoch: 76, loss: 6.80623\n",
      "epoch: 77, loss: 6.80481\n",
      "epoch: 78, loss: 6.80426\n",
      "epoch: 79, loss: 6.80228\n",
      "epoch: 80, loss: 6.80144\n",
      "epoch: 81, loss: 6.80043\n",
      "epoch: 82, loss: 6.79785\n",
      "epoch: 83, loss: 6.79848\n",
      "epoch: 84, loss: 6.79596\n",
      "epoch: 85, loss: 6.79618\n",
      "epoch: 86, loss: 6.79565\n",
      "epoch: 87, loss: 6.79452\n",
      "epoch: 88, loss: 6.79446\n",
      "epoch: 89, loss: 6.79271\n",
      "epoch: 90, loss: 6.79315\n",
      "epoch: 91, loss: 6.78862\n",
      "epoch: 92, loss: 6.79201\n",
      "epoch: 93, loss: 6.78928\n",
      "epoch: 94, loss: 6.78961\n",
      "epoch: 95, loss: 6.78812\n",
      "epoch: 96, loss: 6.78685\n",
      "epoch: 97, loss: 6.78740\n",
      "epoch: 98, loss: 6.78381\n",
      "epoch: 99, loss: 6.78457\n",
      "epoch: 100, loss: 6.78405\n",
      "epoch: 101, loss: 6.78232\n",
      "epoch: 102, loss: 6.78093\n",
      "epoch: 103, loss: 6.78274\n",
      "epoch: 104, loss: 6.78043\n",
      "epoch: 105, loss: 6.78029\n",
      "epoch: 106, loss: 6.77895\n",
      "epoch: 107, loss: 6.77873\n",
      "epoch: 108, loss: 6.77793\n",
      "epoch: 109, loss: 6.77563\n",
      "epoch: 110, loss: 6.77572\n",
      "epoch: 111, loss: 6.77661\n",
      "epoch: 112, loss: 6.77570\n",
      "epoch: 113, loss: 6.77376\n",
      "epoch: 114, loss: 6.77318\n",
      "epoch: 115, loss: 6.77270\n",
      "epoch: 116, loss: 6.77246\n",
      "epoch: 117, loss: 6.77159\n",
      "epoch: 118, loss: 6.77200\n",
      "epoch: 119, loss: 6.77108\n",
      "epoch: 120, loss: 6.77035\n",
      "epoch: 121, loss: 6.76871\n",
      "epoch: 122, loss: 6.76885\n",
      "epoch: 123, loss: 6.76978\n",
      "epoch: 124, loss: 6.76753\n",
      "epoch: 125, loss: 6.76603\n",
      "epoch: 126, loss: 6.76780\n",
      "epoch: 127, loss: 6.76606\n",
      "epoch: 128, loss: 6.76478\n",
      "epoch: 129, loss: 6.76429\n",
      "epoch: 130, loss: 6.76451\n",
      "epoch: 131, loss: 6.76373\n",
      "epoch: 132, loss: 6.76291\n",
      "epoch: 133, loss: 6.76235\n",
      "epoch: 134, loss: 6.76167\n",
      "epoch: 135, loss: 6.76249\n",
      "epoch: 136, loss: 6.76042\n",
      "epoch: 137, loss: 6.76044\n",
      "epoch: 138, loss: 6.76011\n",
      "epoch: 139, loss: 6.75923\n",
      "epoch: 140, loss: 6.75925\n",
      "epoch: 141, loss: 6.75904\n",
      "epoch: 142, loss: 6.75707\n",
      "epoch: 143, loss: 6.75664\n",
      "epoch: 144, loss: 6.75717\n",
      "epoch: 145, loss: 6.75737\n",
      "epoch: 146, loss: 6.75528\n",
      "epoch: 147, loss: 6.75495\n",
      "epoch: 148, loss: 6.75461\n",
      "epoch: 149, loss: 6.75536\n",
      "epoch: 150, loss: 6.75326\n",
      "epoch: 151, loss: 6.75238\n",
      "epoch: 152, loss: 6.75304\n",
      "epoch: 153, loss: 6.75245\n",
      "epoch: 154, loss: 6.75256\n",
      "epoch: 155, loss: 6.75197\n",
      "epoch: 156, loss: 6.75160\n",
      "epoch: 157, loss: 6.75197\n",
      "epoch: 158, loss: 6.74964\n",
      "epoch: 159, loss: 6.75012\n",
      "epoch: 160, loss: 6.75006\n",
      "epoch: 161, loss: 6.74863\n",
      "epoch: 162, loss: 6.74890\n",
      "epoch: 163, loss: 6.74855\n",
      "epoch: 164, loss: 6.74846\n",
      "epoch: 165, loss: 6.74947\n",
      "epoch: 166, loss: 6.74826\n",
      "epoch: 167, loss: 6.74721\n",
      "epoch: 168, loss: 6.74748\n",
      "epoch: 169, loss: 6.74863\n",
      "epoch: 170, loss: 6.74659\n",
      "epoch: 171, loss: 6.74676\n",
      "epoch: 172, loss: 6.74485\n",
      "epoch: 173, loss: 6.74454\n",
      "epoch: 174, loss: 6.74503\n",
      "epoch: 175, loss: 6.74762\n",
      "epoch: 176, loss: 6.74448\n",
      "epoch: 177, loss: 6.74526\n",
      "epoch: 178, loss: 6.74505\n",
      "epoch: 179, loss: 6.74378\n",
      "epoch: 180, loss: 6.74361\n",
      "epoch: 181, loss: 6.74407\n",
      "epoch: 182, loss: 6.74314\n",
      "epoch: 183, loss: 6.74301\n",
      "epoch: 184, loss: 6.74346\n",
      "epoch: 185, loss: 6.74357\n",
      "epoch: 186, loss: 6.74138\n",
      "epoch: 187, loss: 6.74240\n",
      "epoch: 188, loss: 6.74208\n",
      "epoch: 189, loss: 6.74218\n",
      "epoch: 190, loss: 6.74188\n",
      "epoch: 191, loss: 6.74268\n",
      "epoch: 192, loss: 6.74200\n",
      "epoch: 193, loss: 6.74163\n",
      "epoch: 194, loss: 6.74137\n",
      "epoch: 195, loss: 6.74035\n",
      "epoch: 196, loss: 6.73975\n",
      "epoch: 197, loss: 6.74188\n",
      "epoch: 198, loss: 6.74068\n",
      "epoch: 199, loss: 6.73919\n"
     ]
    }
   ],
   "source": [
    "criterion = NTXentLoss(memory_bank_size=(4096, 128))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.06)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    momentum_val = cosine_schedule(epoch, epochs, 0.996, 1)\n",
    "    for batch in dataloader:\n",
    "        x_query, x_key = batch[0]\n",
    "        update_momentum(model.backbone, model.backbone_momentum, m=momentum_val)\n",
    "        update_momentum(\n",
    "            model.projection_head, model.projection_head_momentum, m=momentum_val\n",
    "        )\n",
    "        x_query = x_query.to(device)\n",
    "        x_key = x_key.to(device)\n",
    "        query = model(x_query)\n",
    "        key = model.forward_momentum(x_key)\n",
    "        loss = criterion(query, key)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_backbone  = nn.Sequential(*list(model.backbone.children())[:-1])\n",
    "torch.save(new_backbone.state_dict(), 'models/backbone_weights_200.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        pooled_features = nn.AdaptiveAvgPool2d((1, 1))(features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        output = self.classifier(pooled_features)\n",
    "        return output\n",
    "\n",
    "classification_model = ClassificationNet(new_backbone, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "classification_transform = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop(96),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  # RGB for classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import STL10\n",
    "stl10_train = STL10(root='../data', split='train', download=True, transform=classification_transform)\n",
    "stl10_test = STL10(root='../data', split='test', download=True, transform=classification_transform)\n",
    "\n",
    "# Fine-tuning: Load training data for classification task\n",
    "train_loader = DataLoader(stl10_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Testing: Load test data for final evaluation\n",
    "test_loader = DataLoader(stl10_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 1.1250\n",
      "Epoch [2/150], Loss: 0.7289\n",
      "Epoch [3/150], Loss: 0.5277\n",
      "Epoch [4/150], Loss: 0.3535\n",
      "Epoch [5/150], Loss: 0.1954\n",
      "Epoch [6/150], Loss: 0.1410\n",
      "Epoch [7/150], Loss: 0.1201\n",
      "Epoch [8/150], Loss: 0.0611\n",
      "Epoch [9/150], Loss: 0.0964\n",
      "Epoch [10/150], Loss: 0.0590\n",
      "Epoch [11/150], Loss: 0.0520\n",
      "Epoch [12/150], Loss: 0.0229\n",
      "Epoch [13/150], Loss: 0.0086\n",
      "Epoch [14/150], Loss: 0.0284\n",
      "Epoch [15/150], Loss: 0.0232\n",
      "Epoch [16/150], Loss: 0.0605\n",
      "Epoch [17/150], Loss: 0.0467\n",
      "Epoch [18/150], Loss: 0.0409\n",
      "Epoch [19/150], Loss: 0.0115\n",
      "Epoch [20/150], Loss: 0.0057\n",
      "Epoch [21/150], Loss: 0.0064\n",
      "Epoch [22/150], Loss: 0.0094\n",
      "Epoch [23/150], Loss: 0.0046\n",
      "Epoch [24/150], Loss: 0.0108\n",
      "Epoch [25/150], Loss: 0.1085\n",
      "Epoch [26/150], Loss: 0.0453\n",
      "Epoch [27/150], Loss: 0.0313\n",
      "Epoch [28/150], Loss: 0.0083\n",
      "Epoch [29/150], Loss: 0.0102\n",
      "Epoch [30/150], Loss: 0.0632\n",
      "Epoch [31/150], Loss: 0.0195\n",
      "Epoch [32/150], Loss: 0.0105\n",
      "Epoch [33/150], Loss: 0.0551\n",
      "Epoch [34/150], Loss: 0.0288\n",
      "Epoch [35/150], Loss: 0.0281\n",
      "Epoch [36/150], Loss: 0.0086\n",
      "Epoch [37/150], Loss: 0.0047\n",
      "Epoch [38/150], Loss: 0.0037\n",
      "Epoch [39/150], Loss: 0.0016\n",
      "Epoch [40/150], Loss: 0.0020\n",
      "Epoch [41/150], Loss: 0.0081\n",
      "Epoch [42/150], Loss: 0.1579\n",
      "Epoch [43/150], Loss: 0.1083\n",
      "Epoch [44/150], Loss: 0.0138\n",
      "Epoch [45/150], Loss: 0.0054\n",
      "Epoch [46/150], Loss: 0.0296\n",
      "Epoch [47/150], Loss: 0.0050\n",
      "Epoch [48/150], Loss: 0.0065\n",
      "Epoch [49/150], Loss: 0.0186\n",
      "Epoch [50/150], Loss: 0.0037\n",
      "Epoch [51/150], Loss: 0.0020\n",
      "Epoch [52/150], Loss: 0.0071\n",
      "Epoch [53/150], Loss: 0.0047\n",
      "Epoch [54/150], Loss: 0.0087\n",
      "Epoch [55/150], Loss: 0.0495\n",
      "Epoch [56/150], Loss: 0.0156\n",
      "Epoch [57/150], Loss: 0.0165\n",
      "Epoch [58/150], Loss: 0.0048\n",
      "Epoch [59/150], Loss: 0.0076\n",
      "Epoch [60/150], Loss: 0.0021\n",
      "Epoch [61/150], Loss: 0.0013\n",
      "Epoch [62/150], Loss: 0.0040\n",
      "Epoch [63/150], Loss: 0.0731\n",
      "Epoch [64/150], Loss: 0.0234\n",
      "Epoch [65/150], Loss: 0.0110\n",
      "Epoch [66/150], Loss: 0.0564\n",
      "Epoch [67/150], Loss: 0.0449\n",
      "Epoch [68/150], Loss: 0.0114\n",
      "Epoch [69/150], Loss: 0.0531\n",
      "Epoch [70/150], Loss: 0.0050\n",
      "Epoch [71/150], Loss: 0.0024\n",
      "Epoch [72/150], Loss: 0.0015\n",
      "Epoch [73/150], Loss: 0.0047\n",
      "Epoch [74/150], Loss: 0.0787\n",
      "Epoch [75/150], Loss: 0.0337\n",
      "Epoch [76/150], Loss: 0.0313\n",
      "Epoch [77/150], Loss: 0.0143\n",
      "Epoch [78/150], Loss: 0.0538\n",
      "Epoch [79/150], Loss: 0.0075\n",
      "Epoch [80/150], Loss: 0.0112\n",
      "Epoch [81/150], Loss: 0.0284\n",
      "Epoch [82/150], Loss: 0.0034\n",
      "Epoch [83/150], Loss: 0.0045\n",
      "Epoch [84/150], Loss: 0.0038\n",
      "Epoch [85/150], Loss: 0.0201\n",
      "Epoch [86/150], Loss: 0.0185\n",
      "Epoch [87/150], Loss: 0.0041\n",
      "Epoch [88/150], Loss: 0.0013\n",
      "Epoch [89/150], Loss: 0.0010\n",
      "Epoch [90/150], Loss: 0.0006\n",
      "Epoch [91/150], Loss: 0.0005\n",
      "Epoch [92/150], Loss: 0.0004\n",
      "Epoch [93/150], Loss: 0.0004\n",
      "Epoch [94/150], Loss: 0.0005\n",
      "Epoch [95/150], Loss: 0.0004\n",
      "Epoch [96/150], Loss: 0.0005\n",
      "Epoch [97/150], Loss: 0.0026\n",
      "Epoch [98/150], Loss: 0.0256\n",
      "Epoch [99/150], Loss: 0.0124\n",
      "Epoch [100/150], Loss: 0.0106\n",
      "Epoch [101/150], Loss: 0.0081\n",
      "Epoch [102/150], Loss: 0.0367\n",
      "Epoch [103/150], Loss: 0.0773\n",
      "Epoch [104/150], Loss: 0.0446\n",
      "Epoch [105/150], Loss: 0.0326\n",
      "Epoch [106/150], Loss: 0.0250\n",
      "Epoch [107/150], Loss: 0.0110\n",
      "Epoch [108/150], Loss: 0.0231\n",
      "Epoch [109/150], Loss: 0.0225\n",
      "Epoch [110/150], Loss: 0.0139\n",
      "Epoch [111/150], Loss: 0.0049\n",
      "Epoch [112/150], Loss: 0.0091\n",
      "Epoch [113/150], Loss: 0.0023\n",
      "Epoch [114/150], Loss: 0.0010\n",
      "Epoch [115/150], Loss: 0.0010\n",
      "Epoch [116/150], Loss: 0.0008\n",
      "Epoch [117/150], Loss: 0.0024\n",
      "Epoch [118/150], Loss: 0.0321\n",
      "Epoch [119/150], Loss: 0.0037\n",
      "Epoch [120/150], Loss: 0.0080\n",
      "Epoch [121/150], Loss: 0.0256\n",
      "Epoch [122/150], Loss: 0.0085\n",
      "Epoch [123/150], Loss: 0.0399\n",
      "Epoch [124/150], Loss: 0.0580\n",
      "Epoch [125/150], Loss: 0.0061\n",
      "Epoch [126/150], Loss: 0.0042\n",
      "Epoch [127/150], Loss: 0.0198\n",
      "Epoch [128/150], Loss: 0.0087\n",
      "Epoch [129/150], Loss: 0.0226\n",
      "Epoch [130/150], Loss: 0.0035\n",
      "Epoch [131/150], Loss: 0.0017\n",
      "Epoch [132/150], Loss: 0.0019\n",
      "Epoch [133/150], Loss: 0.0006\n",
      "Epoch [134/150], Loss: 0.0006\n",
      "Epoch [135/150], Loss: 0.0004\n",
      "Epoch [136/150], Loss: 0.0006\n",
      "Epoch [137/150], Loss: 0.0005\n",
      "Epoch [138/150], Loss: 0.0003\n",
      "Epoch [139/150], Loss: 0.0025\n",
      "Epoch [140/150], Loss: 0.0331\n",
      "Epoch [141/150], Loss: 0.0077\n",
      "Epoch [142/150], Loss: 0.0314\n",
      "Epoch [143/150], Loss: 0.0077\n",
      "Epoch [144/150], Loss: 0.0027\n",
      "Epoch [145/150], Loss: 0.0011\n",
      "Epoch [146/150], Loss: 0.0010\n",
      "Epoch [147/150], Loss: 0.0023\n",
      "Epoch [148/150], Loss: 0.0155\n",
      "Epoch [149/150], Loss: 0.0065\n",
      "Epoch [150/150], Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # multi-class classification\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()  \n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = classification_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # if (epoch + 1) % 10 == 0:\n",
    "    #     torch.save(classification_model.state_dict(), f'models/downstream/classification_model_weights_epoch_{epoch+1}.pth')\n",
    "\n",
    "\n",
    "\n",
    "PATH = 'models/downstream/classification_model_weights_final_200.pth'\n",
    "torch.save(classification_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy of the model on the test set: 71.85%\n",
      "Top-5 Accuracy of the model on the test set: 96.89%\n",
      "Top-3 Accuracy of the model on the test set: 91.45%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "classification_model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "top_5_correct = 0\n",
    "top_3_correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        _, predicted_3 = torch.topk(outputs.data, k=3, dim=1)\n",
    "        correct_3 = predicted_3.eq(labels.unsqueeze(1).expand_as(predicted_3))\n",
    "        top_3_correct += correct_3.any(dim=1).sum().item()\n",
    "\n",
    "        _, predicted_5 = torch.topk(outputs.data, k=5, dim=1)\n",
    "        correct_5 = predicted_5.eq(labels.unsqueeze(1).expand_as(predicted_5))\n",
    "        top_5_correct += correct_5.any(dim=1).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top_5 = 100 * top_5_correct / total\n",
    "top_3 = 100 * top_3_correct / total\n",
    "print(f'Top-1 Accuracy of the model on the test set: {accuracy:.2f}%')\n",
    "print(f'Top-5 Accuracy of the model on the test set: {top_5:.2f}%')\n",
    "print(f'Top-3 Accuracy of the model on the test set: {top_3:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
