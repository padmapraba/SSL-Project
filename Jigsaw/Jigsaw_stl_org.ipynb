{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 6000\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Predefine 1000 fixed permutations from the 9! possible permutations for the jigsaw task\n",
    "def create_permutations(num_patches=9, num_permutations=1000):\n",
    "    all_permutations = list(itertools.permutations(range(num_patches)))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    selected_permutations = random.sample(all_permutations, num_permutations)\n",
    "    return selected_permutations\n",
    "\n",
    "permutations = create_permutations()\n",
    "\n",
    "def create_jigsaw_puzzle(image, grid_size=3, permutations=permutations):\n",
    "    _, height, width = image.shape\n",
    "    patch_h, patch_w = height // grid_size, width // grid_size\n",
    "    patches = []\n",
    "\n",
    "    # Extract the patches from the image\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            patch = image[:, i * patch_h: (i + 1) * patch_h, j * patch_w: (j + 1) * patch_w]\n",
    "            patches.append(patch)\n",
    "\n",
    "    # Select a random permutation index from the predefined set\n",
    "    perm_class = random.choice(range(len(permutations)))\n",
    "    perm = permutations[perm_class]\n",
    "\n",
    "    # Shuffle the patches based on the selected permutation\n",
    "    shuffled_patches = [patches[i] for i in perm]\n",
    "    \n",
    "    return torch.stack(shuffled_patches), torch.tensor(perm_class, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset for Jigsaw Pretext Task\n",
    "from torchvision.datasets import STL10\n",
    "class JigsawSTL10Dataset(Dataset):\n",
    "    def __init__(self, dataset, grid_size=3):\n",
    "        self.dataset = dataset\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.dataset[index]\n",
    "        shuffled_patches, perm_class = create_jigsaw_puzzle(img, self.grid_size)\n",
    "        return shuffled_patches, perm_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Add Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize(255),\n",
    "    transforms.RandomHorizontalFlip(),     # Random horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),  # Adjust brightness, contrast, and saturation\n",
    "    transforms.RandomGrayscale(p=0.1),     # Randomly convert to grayscale with a probability of 0.1\n",
    "    transforms.ToTensor(),                 # Convert to tensor\n",
    "])\n",
    "\n",
    "# Dataset and Dataloader\n",
    "train_dataset = STL10(root='./data',  split='train+unlabeled', download=True, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "jigsaw_train_dataset = JigsawSTL10Dataset(train_dataset)\n",
    "jigsaw_val_dataset = JigsawSTL10Dataset(val_dataset)\n",
    "\n",
    "jigsaw_train_loader = DataLoader(jigsaw_train_dataset, batch_size=64, shuffle=True, num_workers=12)\n",
    "jigsaw_val_loader = DataLoader(jigsaw_val_dataset, batch_size=64, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JigsawResNet(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet-based Jigsaw Puzzle Solver\n",
    "class JigsawResNet(nn.Module):\n",
    "    def __init__(self, num_patches, num_permutations):\n",
    "        super(JigsawResNet, self).__init__()\n",
    "        # ResNet-18 backbone\n",
    "        self.backbone = resnet18(weights=None)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])  # Remove last fully connected layer\n",
    "\n",
    "        # Fully connected layer for jigsaw permutation classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * num_patches, 1024),  # 512 is output feature size from ResNet, num_patches = 9\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_permutations)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, num_patches, channels, height, width]\n",
    "        batch_size, num_patches, channels, height, width = x.shape\n",
    "        patches = []\n",
    "\n",
    "        # Process each patch independently\n",
    "        for i in range(num_patches):\n",
    "            patch_features = self.backbone(x[:, i])  # Extract features for each patch\n",
    "            patch_features = torch.flatten(patch_features, start_dim=1)  # Flatten each patch's features\n",
    "            patches.append(patch_features)\n",
    "            # patch = x[:, i]  # Extract the i-th patch: [batch_size, channels, height, width]\n",
    "            # patch_features = self.backbone(patch)  # Pass through backbone\n",
    "            \n",
    "            # patch_features = patch_features.view(batch_size, -1)  # Flatten to [batch_size, 512]\n",
    "            # patches.append(patch_features)\n",
    "\n",
    "        # Concatenate features from all patches\n",
    "        concatenated_features = torch.cat(patches, dim=1)  # Shape: [batch_size, 512 * num_patches]\n",
    "        \n",
    "        # Pass concatenated features through fully connected layers\n",
    "        output = self.fc(concatenated_features)\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_permutations = 1000\n",
    "num_patches = 9\n",
    "\n",
    "model = JigsawResNet(num_patches=num_patches, num_permutations=num_permutations).to(device)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 6.9357\n",
      "Epoch [1/100], Validation Loss: 6.9078\n",
      "Model saved at Epoch 1\n",
      "Epoch [2/100], Training Loss: 6.9082\n",
      "Epoch [2/100], Validation Loss: 6.9059\n",
      "Model saved at Epoch 2\n",
      "Epoch [3/100], Training Loss: 6.3515\n",
      "Epoch [3/100], Validation Loss: 5.2587\n",
      "Model saved at Epoch 3\n",
      "Epoch [4/100], Training Loss: 4.7777\n",
      "Epoch [4/100], Validation Loss: 4.2391\n",
      "Model saved at Epoch 4\n",
      "Epoch [5/100], Training Loss: 4.1442\n",
      "Epoch [5/100], Validation Loss: 3.8913\n",
      "Model saved at Epoch 5\n",
      "Epoch [6/100], Training Loss: 3.8409\n",
      "Epoch [6/100], Validation Loss: 3.6581\n",
      "Model saved at Epoch 6\n",
      "Epoch [7/100], Training Loss: 3.6283\n",
      "Epoch [7/100], Validation Loss: 3.4874\n",
      "Model saved at Epoch 7\n",
      "Epoch [8/100], Training Loss: 3.4455\n",
      "Epoch [8/100], Validation Loss: 3.2870\n",
      "Model saved at Epoch 8\n",
      "Epoch [9/100], Training Loss: 3.2531\n",
      "Epoch [9/100], Validation Loss: 3.1073\n",
      "Model saved at Epoch 9\n",
      "Epoch [10/100], Training Loss: 3.0156\n",
      "Epoch [10/100], Validation Loss: 2.8327\n",
      "Model saved at Epoch 10\n",
      "Epoch [11/100], Training Loss: 2.7790\n",
      "Epoch [11/100], Validation Loss: 2.6583\n",
      "Model saved at Epoch 11\n",
      "Epoch [12/100], Training Loss: 2.5986\n",
      "Epoch [12/100], Validation Loss: 2.4847\n",
      "Model saved at Epoch 12\n",
      "Epoch [13/100], Training Loss: 2.4548\n",
      "Epoch [13/100], Validation Loss: 2.4089\n",
      "Model saved at Epoch 13\n",
      "Epoch [14/100], Training Loss: 2.3445\n",
      "Epoch [14/100], Validation Loss: 2.2967\n",
      "Model saved at Epoch 14\n",
      "Epoch [15/100], Training Loss: 2.2566\n",
      "Epoch [15/100], Validation Loss: 2.2180\n",
      "Model saved at Epoch 15\n",
      "Epoch [16/100], Training Loss: 2.1792\n",
      "Epoch [16/100], Validation Loss: 2.1661\n",
      "Model saved at Epoch 16\n",
      "Epoch [17/100], Training Loss: 2.1126\n",
      "Epoch [17/100], Validation Loss: 2.1917\n",
      "Epoch [18/100], Training Loss: 2.0503\n",
      "Epoch [18/100], Validation Loss: 2.0980\n",
      "Model saved at Epoch 18\n",
      "Epoch [19/100], Training Loss: 1.9929\n",
      "Epoch [19/100], Validation Loss: 2.0166\n",
      "Model saved at Epoch 19\n",
      "Epoch [20/100], Training Loss: 1.9396\n",
      "Epoch [20/100], Validation Loss: 2.0131\n",
      "Model saved at Epoch 20\n",
      "Epoch [21/100], Training Loss: 1.8982\n",
      "Epoch [21/100], Validation Loss: 2.0164\n",
      "Epoch [22/100], Training Loss: 1.8452\n",
      "Epoch [22/100], Validation Loss: 1.9355\n",
      "Model saved at Epoch 22\n",
      "Epoch [23/100], Training Loss: 1.7942\n",
      "Epoch [23/100], Validation Loss: 1.9645\n",
      "Epoch [24/100], Training Loss: 1.7612\n",
      "Epoch [24/100], Validation Loss: 1.9106\n",
      "Model saved at Epoch 24\n",
      "Epoch [25/100], Training Loss: 1.7205\n",
      "Epoch [25/100], Validation Loss: 1.8981\n",
      "Model saved at Epoch 25\n",
      "Epoch [26/100], Training Loss: 1.6809\n",
      "Epoch [26/100], Validation Loss: 1.8731\n",
      "Model saved at Epoch 26\n",
      "Epoch [27/100], Training Loss: 1.6505\n",
      "Epoch [27/100], Validation Loss: 1.8973\n",
      "Epoch [28/100], Training Loss: 1.6125\n",
      "Epoch [28/100], Validation Loss: 1.8821\n",
      "Epoch [29/100], Training Loss: 1.5700\n",
      "Epoch [29/100], Validation Loss: 1.8402\n",
      "Model saved at Epoch 29\n",
      "Epoch [30/100], Training Loss: 1.5467\n",
      "Epoch [30/100], Validation Loss: 1.8318\n",
      "Model saved at Epoch 30\n",
      "Epoch [31/100], Training Loss: 1.5162\n",
      "Epoch [31/100], Validation Loss: 1.8498\n",
      "Epoch [32/100], Training Loss: 1.4818\n",
      "Epoch [32/100], Validation Loss: 1.8155\n",
      "Model saved at Epoch 32\n",
      "Epoch [33/100], Training Loss: 1.4578\n",
      "Epoch [33/100], Validation Loss: 1.8599\n",
      "Epoch [34/100], Training Loss: 1.4302\n",
      "Epoch [34/100], Validation Loss: 1.8295\n",
      "Epoch [35/100], Training Loss: 1.4028\n",
      "Epoch [35/100], Validation Loss: 1.8222\n",
      "Epoch [36/100], Training Loss: 1.3777\n",
      "Epoch [36/100], Validation Loss: 1.8013\n",
      "Model saved at Epoch 36\n",
      "Epoch [37/100], Training Loss: 1.3486\n",
      "Epoch [37/100], Validation Loss: 1.7919\n",
      "Model saved at Epoch 37\n",
      "Epoch [38/100], Training Loss: 1.3319\n",
      "Epoch [38/100], Validation Loss: 1.8385\n",
      "Epoch [39/100], Training Loss: 1.2974\n",
      "Epoch [39/100], Validation Loss: 1.7774\n",
      "Model saved at Epoch 39\n",
      "Epoch [40/100], Training Loss: 1.2768\n",
      "Epoch [40/100], Validation Loss: 1.8180\n",
      "Epoch [41/100], Training Loss: 1.2533\n",
      "Epoch [41/100], Validation Loss: 1.8004\n",
      "Epoch [42/100], Training Loss: 1.2243\n",
      "Epoch [42/100], Validation Loss: 1.8417\n",
      "Epoch [43/100], Training Loss: 1.2050\n",
      "Epoch [43/100], Validation Loss: 1.7838\n",
      "Epoch [44/100], Training Loss: 1.1868\n",
      "Epoch [44/100], Validation Loss: 1.8394\n",
      "Epoch [45/100], Training Loss: 1.1634\n",
      "Epoch [45/100], Validation Loss: 1.8604\n",
      "Epoch [46/100], Training Loss: 1.1444\n",
      "Epoch [46/100], Validation Loss: 1.8998\n",
      "Epoch [47/100], Training Loss: 1.1215\n",
      "Epoch [47/100], Validation Loss: 1.8774\n",
      "Epoch [48/100], Training Loss: 1.1020\n",
      "Epoch [48/100], Validation Loss: 1.8371\n",
      "Epoch [49/100], Training Loss: 1.0790\n",
      "Epoch [49/100], Validation Loss: 1.8769\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = '/users/nladdha/my_env/model/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# Training Jigsaw Model with Validation for Early Stopping\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 100 # Pretext training for 100 epochs\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for batch_idx, (shuffled_patches, perm_class) in enumerate(jigsaw_train_loader):\n",
    "        shuffled_patches, perm_class = shuffled_patches.to(device), perm_class.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(shuffled_patches)\n",
    "        loss = criterion(outputs, perm_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(jigsaw_train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")    \n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_batch, (val_patches, val_perm_class) in enumerate(jigsaw_val_loader):\n",
    "            val_patches, val_perm_class = val_patches.to(device), val_perm_class.to(device)\n",
    "            val_outputs = model(val_patches)\n",
    "            val_loss += criterion(val_outputs, val_perm_class).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(jigsaw_val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Model checkpointing based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model_path = os.path.join(checkpoint_dir, 'stl_jigsaw_model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model saved at Epoch {epoch+1}\")\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    # Early stopping based on validation loss\n",
    "    if no_improve_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping due to no improvement in validation loss.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Classifier for STL-10 (Downstream Task)\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_classes=10):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.backbone = pretrained_model.module.backbone  # Use the backbone from the pre-trained jigsaw model\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten feature map\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load the pre-trained jigsaw model\n",
    "classification_model = ResNetClassifier(model, num_classes=10).to(device)\n",
    "\n",
    "# Freeze the backbone for fine-tuning\n",
    "for param in classification_model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 DataLoader for classification task\n",
    "train_dataset = datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
    "classification_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_dataset = datasets.STL10(root='./data', split='test', download=True, transform=transform)\n",
    "classification_test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 2.0103\n",
      "Classification model saved at Epoch 1\n",
      "Epoch [2/150], Loss: 1.7831\n",
      "Classification model saved at Epoch 2\n",
      "Epoch [3/150], Loss: 1.6614\n",
      "Classification model saved at Epoch 3\n",
      "Epoch [4/150], Loss: 1.6058\n",
      "Classification model saved at Epoch 4\n",
      "Epoch [5/150], Loss: 1.5534\n",
      "Classification model saved at Epoch 5\n",
      "Epoch [6/150], Loss: 1.5446\n",
      "Classification model saved at Epoch 6\n",
      "Epoch [7/150], Loss: 1.5001\n",
      "Classification model saved at Epoch 7\n",
      "Epoch [8/150], Loss: 1.4928\n",
      "Classification model saved at Epoch 8\n",
      "Epoch [9/150], Loss: 1.4673\n",
      "Classification model saved at Epoch 9\n",
      "Epoch [10/150], Loss: 1.4194\n",
      "Classification model saved at Epoch 10\n",
      "Epoch [11/150], Loss: 1.4043\n",
      "Classification model saved at Epoch 11\n",
      "Epoch [12/150], Loss: 1.3988\n",
      "Classification model saved at Epoch 12\n",
      "Epoch [13/150], Loss: 1.3796\n",
      "Classification model saved at Epoch 13\n",
      "Epoch [14/150], Loss: 1.3632\n",
      "Classification model saved at Epoch 14\n",
      "Epoch [15/150], Loss: 1.3611\n",
      "Classification model saved at Epoch 15\n",
      "Epoch [16/150], Loss: 1.3519\n",
      "Classification model saved at Epoch 16\n",
      "Epoch [17/150], Loss: 1.3368\n",
      "Classification model saved at Epoch 17\n",
      "Epoch [18/150], Loss: 1.3308\n",
      "Classification model saved at Epoch 18\n",
      "Epoch [19/150], Loss: 1.3104\n",
      "Classification model saved at Epoch 19\n",
      "Epoch [20/150], Loss: 1.3189\n",
      "Epoch [21/150], Loss: 1.3324\n",
      "Epoch [22/150], Loss: 1.3136\n",
      "Epoch [23/150], Loss: 1.2759\n",
      "Classification model saved at Epoch 23\n",
      "Epoch [24/150], Loss: 1.3057\n",
      "Epoch [25/150], Loss: 1.3026\n",
      "Epoch [26/150], Loss: 1.2858\n",
      "Epoch [27/150], Loss: 1.2856\n",
      "Epoch [28/150], Loss: 1.2541\n",
      "Classification model saved at Epoch 28\n",
      "Epoch [29/150], Loss: 1.2507\n",
      "Classification model saved at Epoch 29\n",
      "Epoch [30/150], Loss: 1.2858\n",
      "Epoch [31/150], Loss: 1.2369\n",
      "Classification model saved at Epoch 31\n",
      "Epoch [32/150], Loss: 1.2372\n",
      "Epoch [33/150], Loss: 1.2501\n",
      "Epoch [34/150], Loss: 1.2124\n",
      "Classification model saved at Epoch 34\n",
      "Epoch [35/150], Loss: 1.2242\n",
      "Epoch [36/150], Loss: 1.2226\n",
      "Epoch [37/150], Loss: 1.2326\n",
      "Epoch [38/150], Loss: 1.2192\n",
      "Epoch [39/150], Loss: 1.2197\n",
      "Epoch [40/150], Loss: 1.2076\n",
      "Classification model saved at Epoch 40\n",
      "Epoch [41/150], Loss: 1.2095\n",
      "Epoch [42/150], Loss: 1.1996\n",
      "Classification model saved at Epoch 42\n",
      "Epoch [43/150], Loss: 1.1844\n",
      "Classification model saved at Epoch 43\n",
      "Epoch [44/150], Loss: 1.1912\n",
      "Epoch [45/150], Loss: 1.2085\n",
      "Epoch [46/150], Loss: 1.1844\n",
      "Classification model saved at Epoch 46\n",
      "Epoch [47/150], Loss: 1.1745\n",
      "Classification model saved at Epoch 47\n",
      "Epoch [48/150], Loss: 1.1764\n",
      "Epoch [49/150], Loss: 1.1676\n",
      "Classification model saved at Epoch 49\n",
      "Epoch [50/150], Loss: 1.1617\n",
      "Classification model saved at Epoch 50\n",
      "Epoch [51/150], Loss: 1.1740\n",
      "Epoch [52/150], Loss: 1.1970\n",
      "Epoch [53/150], Loss: 1.1498\n",
      "Classification model saved at Epoch 53\n",
      "Epoch [54/150], Loss: 1.1384\n",
      "Classification model saved at Epoch 54\n",
      "Epoch [55/150], Loss: 1.1595\n",
      "Epoch [56/150], Loss: 1.1847\n",
      "Epoch [57/150], Loss: 1.1502\n",
      "Epoch [58/150], Loss: 1.1428\n",
      "Epoch [59/150], Loss: 1.1510\n",
      "Epoch [60/150], Loss: 1.1204\n",
      "Classification model saved at Epoch 60\n",
      "Epoch [61/150], Loss: 1.1405\n",
      "Epoch [62/150], Loss: 1.1461\n",
      "Epoch [63/150], Loss: 1.1507\n",
      "Epoch [64/150], Loss: 1.1164\n",
      "Classification model saved at Epoch 64\n",
      "Epoch [65/150], Loss: 1.1182\n",
      "Epoch [66/150], Loss: 1.1030\n",
      "Classification model saved at Epoch 66\n",
      "Epoch [67/150], Loss: 1.1192\n",
      "Epoch [68/150], Loss: 1.0858\n",
      "Classification model saved at Epoch 68\n",
      "Epoch [69/150], Loss: 1.1177\n",
      "Epoch [70/150], Loss: 1.0979\n",
      "Epoch [71/150], Loss: 1.1144\n",
      "Epoch [72/150], Loss: 1.0934\n",
      "Epoch [73/150], Loss: 1.0921\n",
      "Epoch [74/150], Loss: 1.1066\n",
      "Epoch [75/150], Loss: 1.0505\n",
      "Classification model saved at Epoch 75\n",
      "Epoch [76/150], Loss: 1.0866\n",
      "Epoch [77/150], Loss: 1.0699\n",
      "Epoch [78/150], Loss: 1.0731\n",
      "Epoch [79/150], Loss: 1.0656\n",
      "Epoch [80/150], Loss: 1.0789\n",
      "Epoch [81/150], Loss: 1.0757\n",
      "Epoch [82/150], Loss: 1.0692\n",
      "Epoch [83/150], Loss: 1.0652\n",
      "Epoch [84/150], Loss: 1.0488\n",
      "Classification model saved at Epoch 84\n",
      "Epoch [85/150], Loss: 1.0448\n",
      "Classification model saved at Epoch 85\n",
      "Epoch [86/150], Loss: 1.0726\n",
      "Epoch [87/150], Loss: 1.0499\n",
      "Epoch [88/150], Loss: 1.0434\n",
      "Classification model saved at Epoch 88\n",
      "Epoch [89/150], Loss: 1.0177\n",
      "Classification model saved at Epoch 89\n",
      "Epoch [90/150], Loss: 1.0355\n",
      "Epoch [91/150], Loss: 1.0408\n",
      "Epoch [92/150], Loss: 1.0144\n",
      "Classification model saved at Epoch 92\n",
      "Epoch [93/150], Loss: 1.0312\n",
      "Epoch [94/150], Loss: 1.0211\n",
      "Epoch [95/150], Loss: 1.0235\n",
      "Epoch [96/150], Loss: 1.0079\n",
      "Classification model saved at Epoch 96\n",
      "Epoch [97/150], Loss: 1.0283\n",
      "Epoch [98/150], Loss: 0.9972\n",
      "Classification model saved at Epoch 98\n",
      "Epoch [99/150], Loss: 1.0329\n",
      "Epoch [100/150], Loss: 0.9924\n",
      "Classification model saved at Epoch 100\n",
      "Epoch [101/150], Loss: 0.9880\n",
      "Classification model saved at Epoch 101\n",
      "Epoch [102/150], Loss: 1.0227\n",
      "Epoch [103/150], Loss: 1.0002\n",
      "Epoch [104/150], Loss: 1.0106\n",
      "Epoch [105/150], Loss: 0.9776\n",
      "Classification model saved at Epoch 105\n",
      "Epoch [106/150], Loss: 0.9916\n",
      "Epoch [107/150], Loss: 0.9882\n",
      "Epoch [108/150], Loss: 0.9968\n",
      "Epoch [109/150], Loss: 0.9944\n",
      "Epoch [110/150], Loss: 0.9602\n",
      "Classification model saved at Epoch 110\n",
      "Epoch [111/150], Loss: 0.9514\n",
      "Classification model saved at Epoch 111\n",
      "Epoch [112/150], Loss: 0.9621\n",
      "Epoch [113/150], Loss: 0.9623\n",
      "Epoch [114/150], Loss: 0.9618\n",
      "Epoch [115/150], Loss: 0.9694\n",
      "Epoch [116/150], Loss: 0.9573\n",
      "Epoch [117/150], Loss: 0.9435\n",
      "Classification model saved at Epoch 117\n",
      "Epoch [118/150], Loss: 0.9447\n",
      "Epoch [119/150], Loss: 0.9197\n",
      "Classification model saved at Epoch 119\n",
      "Epoch [120/150], Loss: 0.9413\n",
      "Epoch [121/150], Loss: 0.9402\n",
      "Epoch [122/150], Loss: 0.9390\n",
      "Epoch [123/150], Loss: 0.9461\n",
      "Epoch [124/150], Loss: 0.9399\n",
      "Epoch [125/150], Loss: 0.9517\n",
      "Epoch [126/150], Loss: 0.9189\n",
      "Classification model saved at Epoch 126\n",
      "Epoch [127/150], Loss: 0.9146\n",
      "Classification model saved at Epoch 127\n",
      "Epoch [128/150], Loss: 0.9402\n",
      "Epoch [129/150], Loss: 0.9207\n",
      "Epoch [130/150], Loss: 0.9348\n",
      "Epoch [131/150], Loss: 0.9125\n",
      "Classification model saved at Epoch 131\n",
      "Epoch [132/150], Loss: 0.9072\n",
      "Classification model saved at Epoch 132\n",
      "Epoch [133/150], Loss: 0.9453\n",
      "Epoch [134/150], Loss: 0.9364\n",
      "Epoch [135/150], Loss: 0.9413\n",
      "Epoch [136/150], Loss: 0.9227\n",
      "Epoch [137/150], Loss: 0.9052\n",
      "Classification model saved at Epoch 137\n",
      "Epoch [138/150], Loss: 0.9191\n",
      "Epoch [139/150], Loss: 0.8817\n",
      "Classification model saved at Epoch 139\n",
      "Epoch [140/150], Loss: 0.8932\n",
      "Epoch [141/150], Loss: 0.8986\n",
      "Epoch [142/150], Loss: 0.9207\n",
      "Epoch [143/150], Loss: 0.8872\n",
      "Epoch [144/150], Loss: 0.8829\n",
      "Epoch [145/150], Loss: 0.8833\n",
      "Epoch [146/150], Loss: 0.8520\n",
      "Classification model saved at Epoch 146\n",
      "Epoch [147/150], Loss: 0.8632\n",
      "Epoch [148/150], Loss: 0.9113\n",
      "Epoch [149/150], Loss: 0.8739\n",
      "Epoch [150/150], Loss: 0.8572\n",
      "Top-1 Accuracy: 49.86%\n",
      "Top-5 Accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning loop\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "classification_optimizer = optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "classification_epochs = 150\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(classification_epochs):\n",
    "    classification_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(classification_train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        classification_optimizer.zero_grad()\n",
    "        outputs = classification_model(inputs)\n",
    "        loss = classification_criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        classification_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(classification_train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{classification_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Model checkpointing based on validation loss (use validation loader if needed)\n",
    "    if avg_loss < best_val_loss:\n",
    "        best_val_loss = avg_loss\n",
    "        torch.save(classification_model.state_dict(), 'stl_classification_model.pth')\n",
    "        print(f\"Classification model saved at Epoch {epoch+1}\")\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improve_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Evaluation with Top-1 and Top-5 Accuracy\n",
    "classification_model.eval()\n",
    "correct = 0\n",
    "top_5_correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in classification_test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Top-5 accuracy\n",
    "        _, predicted_5 = torch.topk(outputs.data, k=5, dim=1)\n",
    "        top_5_correct += (predicted_5 == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top_5_accuracy = 100 * top_5_correct / total\n",
    "print(f'Top-1 Accuracy: {accuracy:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_5_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
