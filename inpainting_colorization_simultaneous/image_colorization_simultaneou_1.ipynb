{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Import custom modules from provided files\n",
    "from colorization import Colorization, RGB2LabTransform, STL10ColorizationDataset, EarlyStopping\n",
    "from inpainting import Encoder, Decoder, InpaintingModel, Discriminator, mask_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/soh62/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/soh62/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using new encoder\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a combined model class with both colorization and inpainting models\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.colorization_model = Colorization(backbone)\n",
    "        self.inpainting_model = InpaintingModel()\n",
    "\n",
    "    def forward_colorization(self, L_channel_rgb):\n",
    "        return self.colorization_model(L_channel_rgb)\n",
    "\n",
    "    def forward_inpainting(self, masked_images):\n",
    "        return self.inpainting_model(masked_images)\n",
    "\n",
    "# Initialize the backbone for both models\n",
    "backbone = resnet18(pretrained=True)\n",
    "backbone = nn.Sequential(*list(backbone.children())[:-2])  # Removing the last layers for feature extraction\n",
    "combined_model = CombinedModel(backbone).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert LAB to RGB\n",
    "def lab_to_rgb(L_channel, AB_channels):\n",
    "    L_channel = L_channel.squeeze().cpu().numpy() * 255\n",
    "    AB_channels = AB_channels.squeeze().detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    AB_channels = (AB_channels * 255) - 128\n",
    "\n",
    "    lab_image = np.concatenate((L_channel[:, :, np.newaxis], AB_channels), axis=-1)\n",
    "    rgb_image = color.lab2rgb(lab_image)\n",
    "    \n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load STL10 dataset and apply necessary transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Separate datasets for colorization and inpainting\n",
    "stl10_colorization = STL10ColorizationDataset(root='../data', split='train+unlabeled', download=True, transform=transform)\n",
    "stl10_inpainting = STL10(root='../data', split='train+unlabeled', download=True, transform=transform)\n",
    "\n",
    "# DataLoaders for colorization and inpainting tasks\n",
    "colorization_loader = DataLoader(stl10_colorization, batch_size=64, shuffle=True)\n",
    "inpainting_loader = DataLoader(stl10_inpainting, batch_size=64, shuffle=True)\n",
    "\n",
    "# Optimizer and loss functions\n",
    "optimizer = Adam(combined_model.parameters(), lr=1e-3)\n",
    "colorization_criterion = nn.MSELoss()\n",
    "inpainting_criterion = nn.BCELoss()  # Assuming binary cross-entropy loss for inpainting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping configuration\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-6):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "\n",
    "early_stop = EarlyStopping(patience=15, min_delta=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Tasks per Epoch: Each epoch includes both colorization and inpainting tasks back-to-back. The model first processes colorization updates and then inpainting updates within the same epoch, ensuring that both tasks influence each other immediately within each epoch. This setup allows the model to build upon the shared learning of both tasks within a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Colorization Loss: 0.0029\n",
      "Epoch [1/100] Inpainting Loss: 0.5537\n",
      "Epoch [2/100] Colorization Loss: 0.0024\n",
      "Epoch [2/100] Inpainting Loss: 0.5375\n",
      "Epoch [3/100] Colorization Loss: 0.0023\n",
      "Epoch [3/100] Inpainting Loss: 0.5330\n",
      "Epoch [4/100] Colorization Loss: 0.0022\n",
      "Epoch [4/100] Inpainting Loss: 0.5306\n",
      "Epoch [5/100] Colorization Loss: 0.0022\n",
      "Epoch [5/100] Inpainting Loss: 0.5292\n",
      "Epoch [6/100] Colorization Loss: 0.0021\n",
      "Epoch [6/100] Inpainting Loss: 0.5279\n",
      "Epoch [7/100] Colorization Loss: 0.0019\n",
      "Epoch [7/100] Inpainting Loss: 0.5270\n",
      "Epoch [8/100] Colorization Loss: 0.0018\n",
      "Epoch [8/100] Inpainting Loss: 0.5265\n",
      "Epoch [9/100] Colorization Loss: 0.0017\n",
      "Epoch [9/100] Inpainting Loss: 0.5257\n",
      "Epoch [10/100] Colorization Loss: 0.0016\n",
      "Epoch [10/100] Inpainting Loss: 0.5250\n",
      "Epoch [11/100] Colorization Loss: 0.0015\n",
      "Epoch [11/100] Inpainting Loss: 0.5244\n",
      "Epoch [12/100] Colorization Loss: 0.0015\n",
      "Epoch [12/100] Inpainting Loss: 0.5240\n",
      "Epoch [13/100] Colorization Loss: 0.0014\n",
      "Epoch [13/100] Inpainting Loss: 0.5234\n",
      "Epoch [14/100] Colorization Loss: 0.0014\n",
      "Epoch [14/100] Inpainting Loss: 0.5231\n",
      "Epoch [15/100] Colorization Loss: 0.0014\n",
      "Epoch [15/100] Inpainting Loss: 0.5227\n",
      "Epoch [16/100] Colorization Loss: 0.0013\n",
      "Epoch [16/100] Inpainting Loss: 0.5225\n",
      "Epoch [17/100] Colorization Loss: 0.0013\n",
      "Epoch [17/100] Inpainting Loss: 0.5222\n",
      "Epoch [18/100] Colorization Loss: 0.0013\n",
      "Epoch [18/100] Inpainting Loss: 0.5221\n",
      "Epoch [19/100] Colorization Loss: 0.0013\n",
      "Epoch [19/100] Inpainting Loss: 0.5218\n",
      "Epoch [20/100] Colorization Loss: 0.0012\n",
      "Epoch [20/100] Inpainting Loss: 0.5217\n",
      "Epoch [21/100] Colorization Loss: 0.0012\n",
      "Epoch [21/100] Inpainting Loss: 0.5214\n",
      "Epoch [22/100] Colorization Loss: 0.0012\n",
      "Epoch [22/100] Inpainting Loss: 0.5213\n",
      "Epoch [23/100] Colorization Loss: 0.0012\n",
      "Epoch [23/100] Inpainting Loss: 0.5211\n",
      "Epoch [24/100] Colorization Loss: 0.0012\n",
      "Epoch [24/100] Inpainting Loss: 0.5209\n",
      "Epoch [25/100] Colorization Loss: 0.0012\n",
      "Epoch [25/100] Inpainting Loss: 0.5208\n",
      "Epoch [26/100] Colorization Loss: 0.0012\n",
      "Epoch [26/100] Inpainting Loss: 0.5206\n",
      "Epoch [27/100] Colorization Loss: 0.0012\n",
      "Epoch [27/100] Inpainting Loss: 0.5205\n",
      "Epoch [28/100] Colorization Loss: 0.0011\n",
      "Epoch [28/100] Inpainting Loss: 0.5203\n",
      "Epoch [29/100] Colorization Loss: 0.0011\n",
      "Epoch [29/100] Inpainting Loss: 0.5202\n",
      "Epoch [30/100] Colorization Loss: 0.0011\n",
      "Epoch [30/100] Inpainting Loss: 0.5201\n",
      "Epoch [31/100] Colorization Loss: 0.0011\n",
      "Epoch [31/100] Inpainting Loss: 0.5200\n",
      "Epoch [32/100] Colorization Loss: 0.0011\n",
      "Epoch [32/100] Inpainting Loss: 0.5198\n",
      "Epoch [33/100] Colorization Loss: 0.0011\n",
      "Epoch [33/100] Inpainting Loss: 0.5198\n",
      "Epoch [34/100] Colorization Loss: 0.0011\n",
      "Epoch [34/100] Inpainting Loss: 0.5196\n",
      "Epoch [35/100] Colorization Loss: 0.0011\n",
      "Epoch [35/100] Inpainting Loss: 0.5195\n",
      "Epoch [36/100] Colorization Loss: 0.0011\n",
      "Epoch [36/100] Inpainting Loss: 0.5194\n",
      "Epoch [37/100] Colorization Loss: 0.0011\n",
      "Epoch [37/100] Inpainting Loss: 0.5193\n",
      "Epoch [38/100] Colorization Loss: 0.0011\n",
      "Epoch [38/100] Inpainting Loss: 0.5192\n",
      "Epoch [39/100] Colorization Loss: 0.0011\n",
      "Epoch [39/100] Inpainting Loss: 0.5191\n",
      "Epoch [40/100] Colorization Loss: 0.0011\n",
      "Epoch [40/100] Inpainting Loss: 0.5190\n",
      "Epoch [41/100] Colorization Loss: 0.0011\n",
      "Epoch [41/100] Inpainting Loss: 0.5189\n",
      "Epoch [42/100] Colorization Loss: 0.0011\n",
      "Epoch [42/100] Inpainting Loss: 0.5188\n",
      "Epoch [43/100] Colorization Loss: 0.0011\n",
      "Epoch [43/100] Inpainting Loss: 0.5188\n",
      "Epoch [44/100] Colorization Loss: 0.0011\n",
      "Epoch [44/100] Inpainting Loss: 0.5187\n",
      "Epoch [45/100] Colorization Loss: 0.0011\n",
      "Epoch [45/100] Inpainting Loss: 0.5186\n",
      "Epoch [46/100] Colorization Loss: 0.0011\n",
      "Epoch [46/100] Inpainting Loss: 0.5185\n",
      "Epoch [47/100] Colorization Loss: 0.0011\n",
      "Epoch [47/100] Inpainting Loss: 0.5185\n",
      "Epoch [48/100] Colorization Loss: 0.0011\n",
      "Epoch [48/100] Inpainting Loss: 0.5184\n",
      "Epoch [49/100] Colorization Loss: 0.0011\n",
      "Epoch [49/100] Inpainting Loss: 0.5183\n",
      "Epoch [50/100] Colorization Loss: 0.0011\n",
      "Epoch [50/100] Inpainting Loss: 0.5182\n",
      "Epoch [51/100] Colorization Loss: 0.0011\n",
      "Epoch [51/100] Inpainting Loss: 0.5182\n",
      "Epoch [52/100] Colorization Loss: 0.0011\n",
      "Epoch [52/100] Inpainting Loss: 0.5181\n",
      "Epoch [53/100] Colorization Loss: 0.0010\n",
      "Epoch [53/100] Inpainting Loss: 0.5181\n",
      "Epoch [54/100] Colorization Loss: 0.0010\n",
      "Epoch [54/100] Inpainting Loss: 0.5181\n",
      "Epoch [55/100] Colorization Loss: 0.0010\n",
      "Epoch [55/100] Inpainting Loss: 0.5180\n",
      "Epoch [56/100] Colorization Loss: 0.0010\n",
      "Epoch [56/100] Inpainting Loss: 0.5180\n",
      "Epoch [57/100] Colorization Loss: 0.0010\n",
      "Epoch [57/100] Inpainting Loss: 0.5179\n",
      "Epoch [58/100] Colorization Loss: 0.0010\n",
      "Epoch [58/100] Inpainting Loss: 0.5179\n",
      "Epoch [59/100] Colorization Loss: 0.0010\n",
      "Epoch [59/100] Inpainting Loss: 0.5178\n",
      "Epoch [60/100] Colorization Loss: 0.0010\n",
      "Epoch [60/100] Inpainting Loss: 0.5178\n",
      "Epoch [61/100] Colorization Loss: 0.0010\n",
      "Epoch [61/100] Inpainting Loss: 0.5177\n",
      "Epoch [62/100] Colorization Loss: 0.0010\n",
      "Epoch [62/100] Inpainting Loss: 0.5177\n",
      "Epoch [63/100] Colorization Loss: 0.0010\n",
      "Epoch [63/100] Inpainting Loss: 0.5177\n",
      "Epoch [64/100] Colorization Loss: 0.0010\n",
      "Epoch [64/100] Inpainting Loss: 0.5176\n",
      "Epoch [65/100] Colorization Loss: 0.0010\n",
      "Epoch [65/100] Inpainting Loss: 0.5176\n",
      "Epoch [66/100] Colorization Loss: 0.0010\n",
      "Epoch [66/100] Inpainting Loss: 0.5176\n",
      "Epoch [67/100] Colorization Loss: 0.0010\n",
      "Epoch [67/100] Inpainting Loss: 0.5175\n",
      "Epoch [68/100] Colorization Loss: 0.0010\n",
      "Epoch [68/100] Inpainting Loss: 0.5175\n",
      "Epoch [69/100] Colorization Loss: 0.0010\n",
      "Epoch [69/100] Inpainting Loss: 0.5174\n",
      "Epoch [70/100] Colorization Loss: 0.0010\n",
      "Epoch [70/100] Inpainting Loss: 0.5174\n",
      "Epoch [71/100] Colorization Loss: 0.0010\n",
      "Epoch [71/100] Inpainting Loss: 0.5173\n",
      "Epoch [72/100] Colorization Loss: 0.0010\n",
      "Epoch [72/100] Inpainting Loss: 0.5173\n",
      "Epoch [73/100] Colorization Loss: 0.0010\n",
      "Epoch [73/100] Inpainting Loss: 0.5173\n",
      "Epoch [74/100] Colorization Loss: 0.0010\n",
      "Epoch [74/100] Inpainting Loss: 0.5172\n",
      "Epoch [75/100] Colorization Loss: 0.0010\n",
      "Epoch [75/100] Inpainting Loss: 0.5172\n",
      "Epoch [76/100] Colorization Loss: 0.0010\n",
      "Epoch [76/100] Inpainting Loss: 0.5172\n",
      "Epoch [77/100] Colorization Loss: 0.0010\n",
      "Epoch [77/100] Inpainting Loss: 0.5171\n",
      "Epoch [78/100] Colorization Loss: 0.0010\n",
      "Epoch [78/100] Inpainting Loss: 0.5171\n",
      "Epoch [79/100] Colorization Loss: 0.0010\n",
      "Epoch [79/100] Inpainting Loss: 0.5171\n",
      "Epoch [80/100] Colorization Loss: 0.0010\n",
      "Epoch [80/100] Inpainting Loss: 0.5170\n",
      "Epoch [81/100] Colorization Loss: 0.0010\n",
      "Epoch [81/100] Inpainting Loss: 0.5171\n",
      "Epoch [82/100] Colorization Loss: 0.0010\n",
      "Epoch [82/100] Inpainting Loss: 0.5170\n",
      "Epoch [83/100] Colorization Loss: 0.0010\n",
      "Epoch [83/100] Inpainting Loss: 0.5170\n",
      "Epoch [84/100] Colorization Loss: 0.0010\n",
      "Epoch [84/100] Inpainting Loss: 0.5169\n",
      "Epoch [85/100] Colorization Loss: 0.0010\n",
      "Epoch [85/100] Inpainting Loss: 0.5169\n",
      "Epoch [86/100] Colorization Loss: 0.0010\n",
      "Epoch [86/100] Inpainting Loss: 0.5169\n",
      "Epoch [87/100] Colorization Loss: 0.0010\n",
      "Epoch [87/100] Inpainting Loss: 0.5169\n",
      "Epoch [88/100] Colorization Loss: 0.0010\n",
      "Epoch [88/100] Inpainting Loss: 0.5168\n",
      "Epoch [89/100] Colorization Loss: 0.0010\n",
      "Epoch [89/100] Inpainting Loss: 0.5168\n",
      "Epoch [90/100] Colorization Loss: 0.0010\n",
      "Epoch [90/100] Inpainting Loss: 0.5168\n",
      "Epoch [91/100] Colorization Loss: 0.0010\n",
      "Epoch [91/100] Inpainting Loss: 0.5167\n",
      "Epoch [92/100] Colorization Loss: 0.0010\n",
      "Epoch [92/100] Inpainting Loss: 0.5167\n",
      "Epoch [93/100] Colorization Loss: 0.0010\n",
      "Epoch [93/100] Inpainting Loss: 0.5167\n",
      "Epoch [94/100] Colorization Loss: 0.0010\n",
      "Epoch [94/100] Inpainting Loss: 0.5167\n",
      "Epoch [95/100] Colorization Loss: 0.0010\n",
      "Epoch [95/100] Inpainting Loss: 0.5167\n",
      "Epoch [96/100] Colorization Loss: 0.0010\n",
      "Epoch [96/100] Inpainting Loss: 0.5167\n",
      "Epoch [97/100] Colorization Loss: 0.0010\n",
      "Epoch [97/100] Inpainting Loss: 0.5166\n",
      "Epoch [98/100] Colorization Loss: 0.0010\n",
      "Epoch [98/100] Inpainting Loss: 0.5166\n",
      "Epoch [99/100] Colorization Loss: 0.0010\n",
      "Epoch [99/100] Inpainting Loss: 0.5166\n",
      "Epoch [100/100] Colorization Loss: 0.0010\n",
      "Epoch [100/100] Inpainting Loss: 0.5166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Both Tasks per Epoch: Each epoch includes both colorization and inpainting tasks back-to-back. \n",
    "The model first processes colorization updates and then inpainting updates within the same epoch, ensuring that both tasks influence each other immediately within each epoch. \n",
    "This setup allows the model to build upon the shared learning of both tasks within a single epoch.\n",
    "'''\n",
    "# # Set mask size for inpainting\n",
    "# mask_size = 8  # Adjust as needed\n",
    "\n",
    "# # Training loop with combined pretext tasks per epoch\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     combined_model.train()\n",
    "#     total_loss_colorization = 0.0\n",
    "#     total_loss_inpainting = 0.0\n",
    "\n",
    "#     # Colorization task within each epoch\n",
    "#     for L_channel, AB_channels in colorization_loader:\n",
    "#         L_channel, AB_channels = L_channel.to(device), AB_channels.to(device)\n",
    "#         L_channel_rgb = L_channel.repeat(1, 3, 1, 1)  # Convert grayscale to RGB input format\n",
    "\n",
    "#         # Forward pass for colorization\n",
    "#         predicted_AB = combined_model.forward_colorization(L_channel_rgb)\n",
    "#         loss_colorization = colorization_criterion(predicted_AB, AB_channels)\n",
    "\n",
    "#         # Backward pass and optimization for colorization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_colorization.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss_colorization += loss_colorization.item()\n",
    "\n",
    "#     avg_loss_colorization = total_loss_colorization / len(colorization_loader)\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}] Colorization Loss: {avg_loss_colorization:.4f}')\n",
    "    \n",
    "#     # Inpainting task within the same epoch\n",
    "#     for images, _ in inpainting_loader:\n",
    "#         images = images.to(device)\n",
    "#         # Apply masking with specified mask_size\n",
    "#         masked_images, masks = mask_image(images, mask_size=mask_size)\n",
    "#         masked_images, masks = masked_images.to(device), masks.to(device)\n",
    "\n",
    "#         # Forward pass for inpainting\n",
    "#         reconstructed_images = combined_model.forward_inpainting(masked_images)\n",
    "#         loss_inpainting = inpainting_criterion(reconstructed_images, images)\n",
    "\n",
    "#         # Backward pass and optimization for inpainting\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_inpainting.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss_inpainting += loss_inpainting.item()\n",
    "\n",
    "#     avg_loss_inpainting = total_loss_inpainting / len(inpainting_loader)\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}] Inpainting Loss: {avg_loss_inpainting:.4f}')\n",
    "\n",
    "#     # Early stopping check based on combined average loss\n",
    "#     combined_avg_loss = (avg_loss_colorization + avg_loss_inpainting) / 2\n",
    "#     early_stop(combined_avg_loss)\n",
    "#     if early_stop.early_stop:\n",
    "#         print(\"Early Stopping Triggered\")\n",
    "#         break\n",
    "\n",
    "# # Save the final combined model\n",
    "# torch.save(combined_model.state_dict(), 'models/combined_inpaint_colorization_final.pth')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Help by ChatGPT\n",
    "Explanation of the Code\n",
    "Alternating Tasks per Batch: Each epoch now alternates between colorization and inpainting tasks on a batch-by-batch basis. \n",
    "This way, the model receives an update from a colorization batch, which immediately influences the inpainting batch that follows within the same epoch.\n",
    "\n",
    "Iterating Over Both Loaders Simultaneously: We use two iterators (colorization_iter and inpainting_iter) and call next() on each loader to retrieve batches for both tasks in each iteration.\n",
    "\n",
    "Immediate Influence: With this setup, colorization gradients are applied before running the inpainting task in each iteration, allowing immediate influence from colorization to inpainting within the same epoch.\n",
    "\n",
    "'''\n",
    "\n",
    "# Set mask size for inpainting\n",
    "mask_size = 8  # Adjust as needed\n",
    "\n",
    "# Training loop with combined pretext tasks per epoch\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    total_loss_colorization = 0.0\n",
    "    total_loss_inpainting = 0.0\n",
    "\n",
    "    colorization_iter = iter(colorization_loader)\n",
    "    inpainting_iter = iter(inpainting_loader)\n",
    "\n",
    "    # Alternate between colorization and inpainting batches within the epoch\n",
    "    for i in range(min(len(colorization_loader), len(inpainting_loader))):\n",
    "        # Colorization task\n",
    "        try:\n",
    "            L_channel, AB_channels = next(colorization_iter)\n",
    "            L_channel, AB_channels = L_channel.to(device), AB_channels.to(device)\n",
    "            L_channel_rgb = L_channel.repeat(1, 3, 1, 1)  # Convert grayscale to RGB input format\n",
    "\n",
    "            # Forward pass for colorization\n",
    "            predicted_AB = combined_model.forward_colorization(L_channel_rgb)\n",
    "            loss_colorization = colorization_criterion(predicted_AB, AB_channels)\n",
    "\n",
    "            # Backward pass and optimization for colorization\n",
    "            optimizer.zero_grad()\n",
    "            loss_colorization.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_colorization += loss_colorization.item()\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "        # Inpainting task\n",
    "        try:\n",
    "            images, _ = next(inpainting_iter)\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Apply masking with specified mask_size\n",
    "            masked_images, masks = mask_image(images, mask_size=mask_size)\n",
    "            masked_images, masks = masked_images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass for inpainting\n",
    "            reconstructed_images = combined_model.forward_inpainting(masked_images)\n",
    "            loss_inpainting = inpainting_criterion(reconstructed_images, images)\n",
    "\n",
    "            # Backward pass and optimization for inpainting\n",
    "            optimizer.zero_grad()\n",
    "            loss_inpainting.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_inpainting += loss_inpainting.item()\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    # Calculate average losses for the epoch\n",
    "    avg_loss_colorization = total_loss_colorization / len(colorization_loader)\n",
    "    avg_loss_inpainting = total_loss_inpainting / len(inpainting_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Colorization Loss: {avg_loss_colorization:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Inpainting Loss: {avg_loss_inpainting:.4f}')\n",
    "\n",
    "    # Early stopping check based on combined average loss\n",
    "    combined_avg_loss = (avg_loss_colorization + avg_loss_inpainting) / 2\n",
    "    early_stop(combined_avg_loss)\n",
    "    if early_stop.early_stop:\n",
    "        print(\"Early Stopping Triggered\")\n",
    "        break\n",
    "\n",
    "# Save the final combined model\n",
    "torch.save(combined_model.state_dict(), 'combined_inpaint_colorization_fina_1.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function to verify colorization predictions\n",
    "def visualize_colorization(L_channel, predicted_AB, ground_truth_AB):\n",
    "    batch_size = L_channel.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Convert model's output (predicted_AB) to RGB for each sample in the batch\n",
    "        colorized_image = lab_to_rgb(L_channel[i], predicted_AB[i])\n",
    "\n",
    "        # Convert the ground truth to RGB for each sample\n",
    "        ground_truth_rgb = lab_to_rgb(L_channel[i], ground_truth_AB[i])\n",
    "\n",
    "        # Display the colorized image and the ground truth (visualization code)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(colorized_image)\n",
    "        plt.title('Predicted Colorization')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(ground_truth_rgb)\n",
    "        plt.title('Ground Truth')\n",
    "\n",
    "        plt.show()\n",
    "        print(f\"Predicted AB min: {predicted_AB[i].min():.2f}, max: {predicted_AB[i].max():.2f}\")\n",
    "        print(f\"Ground Truth AB min: {ground_truth_AB[i].min():.2f}, max: {ground_truth_AB[i].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream epoch 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Linear(512, num_classes)  # Adjust input size based on backbone output\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        pooled_features = nn.AdaptiveAvgPool2d((1, 1))(features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        output = self.classifier(pooled_features)\n",
    "        return output\n",
    "\n",
    "# Initialize the classification model with the backbone from your trained combined model\n",
    "classification_model = ClassificationNet(combined_model.colorization_model.backbone, num_classes=10).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),  # RGB for classification\n",
    "])\n",
    "\n",
    "# Load STL10 train and test datasets\n",
    "stl10_train = STL10(root='../data', split='train', download=True, transform=classification_transform)\n",
    "stl10_test = STL10(root='../data', split='test', download=True, transform=classification_transform)\n",
    "\n",
    "# DataLoaders for training and testing\n",
    "train_loader = DataLoader(stl10_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(stl10_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.7707\n",
      "Epoch [2/50], Loss: 0.7277\n",
      "Epoch [3/50], Loss: 0.2356\n",
      "Epoch [4/50], Loss: 0.0495\n",
      "Epoch [5/50], Loss: 0.0140\n",
      "Epoch [6/50], Loss: 0.0066\n",
      "Epoch [7/50], Loss: 0.0089\n",
      "Epoch [8/50], Loss: 0.0116\n",
      "Epoch [9/50], Loss: 0.0168\n",
      "Epoch [10/50], Loss: 0.0162\n",
      "Epoch [11/50], Loss: 0.0077\n",
      "Epoch [12/50], Loss: 0.0114\n",
      "Epoch [13/50], Loss: 0.0125\n",
      "Epoch [14/50], Loss: 0.0243\n",
      "Epoch [15/50], Loss: 0.0248\n",
      "Epoch [16/50], Loss: 0.0215\n",
      "Epoch [17/50], Loss: 0.0239\n",
      "Epoch [18/50], Loss: 0.0175\n",
      "Epoch [19/50], Loss: 0.0199\n",
      "Epoch [20/50], Loss: 0.0045\n",
      "Epoch [21/50], Loss: 0.0030\n",
      "Epoch [22/50], Loss: 0.0047\n",
      "Epoch [23/50], Loss: 0.0126\n",
      "Epoch [24/50], Loss: 0.0287\n",
      "Epoch [25/50], Loss: 0.0338\n",
      "Epoch [26/50], Loss: 0.0288\n",
      "Epoch [27/50], Loss: 0.0102\n",
      "Epoch [28/50], Loss: 0.0124\n",
      "Epoch [29/50], Loss: 0.0053\n",
      "Epoch [30/50], Loss: 0.0095\n",
      "Epoch [31/50], Loss: 0.0068\n",
      "Epoch [32/50], Loss: 0.0067\n",
      "Epoch [33/50], Loss: 0.0044\n",
      "Epoch [34/50], Loss: 0.0168\n",
      "Epoch [35/50], Loss: 0.0118\n",
      "Epoch [36/50], Loss: 0.0028\n",
      "Epoch [37/50], Loss: 0.0017\n",
      "Epoch [38/50], Loss: 0.0007\n",
      "Epoch [39/50], Loss: 0.0029\n",
      "Epoch [40/50], Loss: 0.0121\n",
      "Epoch [41/50], Loss: 0.0015\n",
      "Epoch [42/50], Loss: 0.0009\n",
      "Epoch [43/50], Loss: 0.0005\n",
      "Epoch [44/50], Loss: 0.0006\n",
      "Epoch [45/50], Loss: 0.0013\n",
      "Epoch [46/50], Loss: 0.0004\n",
      "Epoch [47/50], Loss: 0.0083\n",
      "Epoch [48/50], Loss: 0.0438\n",
      "Epoch [49/50], Loss: 0.0410\n",
      "Epoch [50/50], Loss: 0.0109\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer for classification\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop for downstream classification task\n",
    "num_epochs = 50  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = classification_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Optionally save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(classification_model.state_dict(), f'models/classification_model_weights_epoch_{epoch+1}.pth')\n",
    "\n",
    "# Save the final classification model\n",
    "torch.save(classification_model.state_dict(), 'models/classification_model_weights_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 71.54%\n",
      "Top-3 Accuracy: 91.64%\n",
      "Top-5 Accuracy: 96.90%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "classification_model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "top_3_correct = 0\n",
    "top_5_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Top-3 and Top-5 accuracy calculations\n",
    "        _, predicted_3 = torch.topk(outputs, k=3, dim=1)\n",
    "        correct_3 = predicted_3.eq(labels.view(-1, 1).expand_as(predicted_3))\n",
    "        top_3_correct += correct_3.any(dim=1).sum().item()\n",
    "\n",
    "        _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
    "        correct_5 = predicted_5.eq(labels.view(-1, 1).expand_as(predicted_5))\n",
    "        top_5_correct += correct_5.any(dim=1).sum().item()\n",
    "\n",
    "# Calculate and print accuracies\n",
    "accuracy = 100 * correct / total\n",
    "top_3_accuracy = 100 * top_3_correct / total\n",
    "top_5_accuracy = 100 * top_5_correct / total\n",
    "print(f'Top-1 Accuracy: {accuracy:.2f}%')\n",
    "print(f'Top-3 Accuracy: {top_3_accuracy:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_5_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Epoch 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Linear(512, num_classes)  # Adjust input size based on backbone output\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        pooled_features = nn.AdaptiveAvgPool2d((1, 1))(features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        output = self.classifier(pooled_features)\n",
    "        return output\n",
    "\n",
    "# Initialize the classification model with the backbone from your trained combined model\n",
    "classification_model = ClassificationNet(combined_model.colorization_model.backbone, num_classes=10).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),  # RGB for classification\n",
    "])\n",
    "\n",
    "# Load STL10 train and test datasets\n",
    "stl10_train = STL10(root='../data', split='train', download=True, transform=classification_transform)\n",
    "stl10_test = STL10(root='../data', split='test', download=True, transform=classification_transform)\n",
    "\n",
    "# DataLoaders for training and testing\n",
    "train_loader = DataLoader(stl10_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(stl10_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3744\n",
      "Epoch [2/100], Loss: 0.0170\n",
      "Epoch [3/100], Loss: 0.0082\n",
      "Epoch [4/100], Loss: 0.0080\n",
      "Epoch [5/100], Loss: 0.0077\n",
      "Epoch [6/100], Loss: 0.0040\n",
      "Epoch [7/100], Loss: 0.0036\n",
      "Epoch [8/100], Loss: 0.0105\n",
      "Epoch [9/100], Loss: 0.0164\n",
      "Epoch [10/100], Loss: 0.0043\n",
      "Epoch [11/100], Loss: 0.0102\n",
      "Epoch [12/100], Loss: 0.0521\n",
      "Epoch [13/100], Loss: 0.0160\n",
      "Epoch [14/100], Loss: 0.0236\n",
      "Epoch [15/100], Loss: 0.0127\n",
      "Epoch [16/100], Loss: 0.0029\n",
      "Epoch [17/100], Loss: 0.0015\n",
      "Epoch [18/100], Loss: 0.0017\n",
      "Epoch [19/100], Loss: 0.0009\n",
      "Epoch [20/100], Loss: 0.0077\n",
      "Epoch [21/100], Loss: 0.0217\n",
      "Epoch [22/100], Loss: 0.0128\n",
      "Epoch [23/100], Loss: 0.0075\n",
      "Epoch [24/100], Loss: 0.0035\n",
      "Epoch [25/100], Loss: 0.0015\n",
      "Epoch [26/100], Loss: 0.0006\n",
      "Epoch [27/100], Loss: 0.0011\n",
      "Epoch [28/100], Loss: 0.0153\n",
      "Epoch [29/100], Loss: 0.0252\n",
      "Epoch [30/100], Loss: 0.0263\n",
      "Epoch [31/100], Loss: 0.0060\n",
      "Epoch [32/100], Loss: 0.0019\n",
      "Epoch [33/100], Loss: 0.0088\n",
      "Epoch [34/100], Loss: 0.0016\n",
      "Epoch [35/100], Loss: 0.0016\n",
      "Epoch [36/100], Loss: 0.0060\n",
      "Epoch [37/100], Loss: 0.0019\n",
      "Epoch [38/100], Loss: 0.0004\n",
      "Epoch [39/100], Loss: 0.0005\n",
      "Epoch [40/100], Loss: 0.0140\n",
      "Epoch [41/100], Loss: 0.0466\n",
      "Epoch [42/100], Loss: 0.0334\n",
      "Epoch [43/100], Loss: 0.0123\n",
      "Epoch [44/100], Loss: 0.0025\n",
      "Epoch [45/100], Loss: 0.0096\n",
      "Epoch [46/100], Loss: 0.0189\n",
      "Epoch [47/100], Loss: 0.0126\n",
      "Epoch [48/100], Loss: 0.0015\n",
      "Epoch [49/100], Loss: 0.0128\n",
      "Epoch [50/100], Loss: 0.0136\n",
      "Epoch [51/100], Loss: 0.0083\n",
      "Epoch [52/100], Loss: 0.0021\n",
      "Epoch [53/100], Loss: 0.0007\n",
      "Epoch [54/100], Loss: 0.0004\n",
      "Epoch [55/100], Loss: 0.0006\n",
      "Epoch [56/100], Loss: 0.0054\n",
      "Epoch [57/100], Loss: 0.0061\n",
      "Epoch [58/100], Loss: 0.0025\n",
      "Epoch [59/100], Loss: 0.0009\n",
      "Epoch [60/100], Loss: 0.0042\n",
      "Epoch [61/100], Loss: 0.0468\n",
      "Epoch [62/100], Loss: 0.0053\n",
      "Epoch [63/100], Loss: 0.0014\n",
      "Epoch [64/100], Loss: 0.0009\n",
      "Epoch [65/100], Loss: 0.0033\n",
      "Epoch [66/100], Loss: 0.0100\n",
      "Epoch [67/100], Loss: 0.0028\n",
      "Epoch [68/100], Loss: 0.0165\n",
      "Epoch [69/100], Loss: 0.0286\n",
      "Epoch [70/100], Loss: 0.0130\n",
      "Epoch [71/100], Loss: 0.0096\n",
      "Epoch [72/100], Loss: 0.0047\n",
      "Epoch [73/100], Loss: 0.0134\n",
      "Epoch [74/100], Loss: 0.0453\n",
      "Epoch [75/100], Loss: 0.0199\n",
      "Epoch [76/100], Loss: 0.0037\n",
      "Epoch [77/100], Loss: 0.0022\n",
      "Epoch [78/100], Loss: 0.0024\n",
      "Epoch [79/100], Loss: 0.0019\n",
      "Epoch [80/100], Loss: 0.0130\n",
      "Epoch [81/100], Loss: 0.0026\n",
      "Epoch [82/100], Loss: 0.0021\n",
      "Epoch [83/100], Loss: 0.0040\n",
      "Epoch [84/100], Loss: 0.0074\n",
      "Epoch [85/100], Loss: 0.0156\n",
      "Epoch [86/100], Loss: 0.0416\n",
      "Epoch [87/100], Loss: 0.0274\n",
      "Epoch [88/100], Loss: 0.0180\n",
      "Epoch [89/100], Loss: 0.0160\n",
      "Epoch [90/100], Loss: 0.0112\n",
      "Epoch [91/100], Loss: 0.0153\n",
      "Epoch [92/100], Loss: 0.0128\n",
      "Epoch [93/100], Loss: 0.0155\n",
      "Epoch [94/100], Loss: 0.0243\n",
      "Epoch [95/100], Loss: 0.0326\n",
      "Epoch [96/100], Loss: 0.0044\n",
      "Epoch [97/100], Loss: 0.0019\n",
      "Epoch [98/100], Loss: 0.0009\n",
      "Epoch [99/100], Loss: 0.0006\n",
      "Epoch [100/100], Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer for classification\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop for downstream classification task\n",
    "num_epochs = 100  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = classification_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Optionally save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(classification_model.state_dict(), f'models/classification_model_weights_epoch_150_{epoch+1}.pth')\n",
    "\n",
    "# Save the final classification model\n",
    "torch.save(classification_model.state_dict(), 'models/classification_model_weights_final_epoch150.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 70.92%\n",
      "Top-3 Accuracy: 90.30%\n",
      "Top-5 Accuracy: 95.60%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "classification_model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "top_3_correct = 0\n",
    "top_5_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Top-3 and Top-5 accuracy calculations\n",
    "        _, predicted_3 = torch.topk(outputs, k=3, dim=1)\n",
    "        correct_3 = predicted_3.eq(labels.view(-1, 1).expand_as(predicted_3))\n",
    "        top_3_correct += correct_3.any(dim=1).sum().item()\n",
    "\n",
    "        _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
    "        correct_5 = predicted_5.eq(labels.view(-1, 1).expand_as(predicted_5))\n",
    "        top_5_correct += correct_5.any(dim=1).sum().item()\n",
    "\n",
    "# Calculate and print accuracies\n",
    "accuracy = 100 * correct / total\n",
    "top_3_accuracy = 100 * top_3_correct / total\n",
    "top_5_accuracy = 100 * top_5_correct / total\n",
    "print(f'Top-1 Accuracy: {accuracy:.2f}%')\n",
    "print(f'Top-3 Accuracy: {top_3_accuracy:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_5_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
