{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Import custom modules from provided files\n",
    "from colorization import Colorization, RGB2LabTransform, STL10ColorizationDataset, EarlyStopping\n",
    "from inpainting import Encoder, Decoder, InpaintingModel, Discriminator, mask_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/soh62/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/soh62/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pre-trained encoder\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a combined model class with both colorization and inpainting models\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, shared_encoder):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = shared_encoder  # Shared encoder for both tasks\n",
    "        self.colorization_head = Colorization(self.encoder)\n",
    "        self.inpainting_head = InpaintingModel(self.encoder)\n",
    "\n",
    "    def forward_colorization(self, L_channel_rgb):\n",
    "        # Pass through colorization head\n",
    "        return self.colorization_head(L_channel_rgb)\n",
    "\n",
    "    def forward_inpainting(self, masked_images):\n",
    "        # Pass through inpainting head\n",
    "        return self.inpainting_head(masked_images)\n",
    "\n",
    "\n",
    "# Initialize a shared encoder (ResNet18 backbone as an example)\n",
    "shared_encoder = resnet18(pretrained=True)\n",
    "shared_encoder = nn.Sequential(*list(shared_encoder.children())[:-2])  # Remove final layers for feature extraction\n",
    "\n",
    "# Initialize the combined model with shared encoder\n",
    "combined_model = CombinedModel(shared_encoder).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert LAB to RGB\n",
    "def lab_to_rgb(L_channel, AB_channels):\n",
    "    L_channel = L_channel.squeeze().cpu().numpy() * 255\n",
    "    AB_channels = AB_channels.squeeze().detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    AB_channels = (AB_channels * 255) - 128\n",
    "\n",
    "    lab_image = np.concatenate((L_channel[:, :, np.newaxis], AB_channels), axis=-1)\n",
    "    rgb_image = color.lab2rgb(lab_image)\n",
    "    \n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load STL10 dataset and apply necessary transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Separate datasets for colorization and inpainting\n",
    "stl10_colorization = STL10ColorizationDataset(root='../data', split='train+unlabeled', download=True, transform=transform)\n",
    "stl10_inpainting = STL10(root='../data', split='train+unlabeled', download=True, transform=transform)\n",
    "\n",
    "# DataLoaders for colorization and inpainting tasks\n",
    "colorization_loader = DataLoader(stl10_colorization, batch_size=64, shuffle=True)\n",
    "inpainting_loader = DataLoader(stl10_inpainting, batch_size=64, shuffle=True)\n",
    "\n",
    "# Optimizer and loss functions\n",
    "optimizer = Adam(combined_model.parameters(), lr=1e-3)\n",
    "colorization_criterion = nn.MSELoss()\n",
    "inpainting_criterion = nn.BCELoss()  # Assuming binary cross-entropy loss for inpainting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping configuration\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-6):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "\n",
    "early_stop = EarlyStopping(patience=15, min_delta=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Colorization Loss: 0.0031\n",
      "Epoch [1/100] Inpainting Loss: 0.5606\n",
      "Epoch [2/100] Colorization Loss: 0.0025\n",
      "Epoch [2/100] Inpainting Loss: 0.5426\n",
      "Epoch [3/100] Colorization Loss: 0.0024\n",
      "Epoch [3/100] Inpainting Loss: 0.5370\n",
      "Epoch [4/100] Colorization Loss: 0.0024\n",
      "Epoch [4/100] Inpainting Loss: 0.5329\n",
      "Epoch [5/100] Colorization Loss: 0.0024\n",
      "Epoch [5/100] Inpainting Loss: 0.5307\n",
      "Epoch [6/100] Colorization Loss: 0.0023\n",
      "Epoch [6/100] Inpainting Loss: 0.5291\n",
      "Epoch [7/100] Colorization Loss: 0.0023\n",
      "Epoch [7/100] Inpainting Loss: 0.5277\n",
      "Epoch [8/100] Colorization Loss: 0.0023\n",
      "Epoch [8/100] Inpainting Loss: 0.5267\n",
      "Epoch [9/100] Colorization Loss: 0.0023\n",
      "Epoch [9/100] Inpainting Loss: 0.5258\n",
      "Epoch [10/100] Colorization Loss: 0.0023\n",
      "Epoch [10/100] Inpainting Loss: 0.5250\n",
      "Epoch [11/100] Colorization Loss: 0.0023\n",
      "Epoch [11/100] Inpainting Loss: 0.5242\n",
      "Epoch [12/100] Colorization Loss: 0.0022\n",
      "Epoch [12/100] Inpainting Loss: 0.5236\n",
      "Epoch [13/100] Colorization Loss: 0.0022\n",
      "Epoch [13/100] Inpainting Loss: 0.5232\n",
      "Epoch [14/100] Colorization Loss: 0.0021\n",
      "Epoch [14/100] Inpainting Loss: 0.5228\n",
      "Epoch [15/100] Colorization Loss: 0.0021\n",
      "Epoch [15/100] Inpainting Loss: 0.5224\n",
      "Epoch [16/100] Colorization Loss: 0.0020\n",
      "Epoch [16/100] Inpainting Loss: 0.5221\n",
      "Epoch [17/100] Colorization Loss: 0.0020\n",
      "Epoch [17/100] Inpainting Loss: 0.5218\n",
      "Epoch [18/100] Colorization Loss: 0.0019\n",
      "Epoch [18/100] Inpainting Loss: 0.5216\n",
      "Epoch [19/100] Colorization Loss: 0.0019\n",
      "Epoch [19/100] Inpainting Loss: 0.5214\n",
      "Epoch [20/100] Colorization Loss: 0.0018\n",
      "Epoch [20/100] Inpainting Loss: 0.5212\n",
      "Epoch [21/100] Colorization Loss: 0.0018\n",
      "Epoch [21/100] Inpainting Loss: 0.5210\n",
      "Epoch [22/100] Colorization Loss: 0.0018\n",
      "Epoch [22/100] Inpainting Loss: 0.5209\n",
      "Epoch [23/100] Colorization Loss: 0.0017\n",
      "Epoch [23/100] Inpainting Loss: 0.5208\n",
      "Epoch [24/100] Colorization Loss: 0.0017\n",
      "Epoch [24/100] Inpainting Loss: 0.5206\n",
      "Epoch [25/100] Colorization Loss: 0.0017\n",
      "Epoch [25/100] Inpainting Loss: 0.5205\n",
      "Epoch [26/100] Colorization Loss: 0.0016\n",
      "Epoch [26/100] Inpainting Loss: 0.5203\n",
      "Epoch [27/100] Colorization Loss: 0.0016\n",
      "Epoch [27/100] Inpainting Loss: 0.5202\n",
      "Epoch [28/100] Colorization Loss: 0.0016\n",
      "Epoch [28/100] Inpainting Loss: 0.5201\n",
      "Epoch [29/100] Colorization Loss: 0.0016\n",
      "Epoch [29/100] Inpainting Loss: 0.5200\n",
      "Epoch [30/100] Colorization Loss: 0.0016\n",
      "Epoch [30/100] Inpainting Loss: 0.5199\n",
      "Epoch [31/100] Colorization Loss: 0.0015\n",
      "Epoch [31/100] Inpainting Loss: 0.5198\n",
      "Epoch [32/100] Colorization Loss: 0.0015\n",
      "Epoch [32/100] Inpainting Loss: 0.5198\n",
      "Epoch [33/100] Colorization Loss: 0.0015\n",
      "Epoch [33/100] Inpainting Loss: 0.5196\n",
      "Epoch [34/100] Colorization Loss: 0.0015\n",
      "Epoch [34/100] Inpainting Loss: 0.5196\n",
      "Epoch [35/100] Colorization Loss: 0.0015\n",
      "Epoch [35/100] Inpainting Loss: 0.5195\n",
      "Epoch [36/100] Colorization Loss: 0.0015\n",
      "Epoch [36/100] Inpainting Loss: 0.5194\n",
      "Epoch [37/100] Colorization Loss: 0.0014\n",
      "Epoch [37/100] Inpainting Loss: 0.5193\n",
      "Epoch [38/100] Colorization Loss: 0.0014\n",
      "Epoch [38/100] Inpainting Loss: 0.5192\n",
      "Epoch [39/100] Colorization Loss: 0.0014\n",
      "Epoch [39/100] Inpainting Loss: 0.5192\n",
      "Epoch [40/100] Colorization Loss: 0.0014\n",
      "Epoch [40/100] Inpainting Loss: 0.5191\n",
      "Epoch [41/100] Colorization Loss: 0.0014\n",
      "Epoch [41/100] Inpainting Loss: 0.5191\n",
      "Epoch [42/100] Colorization Loss: 0.0014\n",
      "Epoch [42/100] Inpainting Loss: 0.5190\n",
      "Epoch [43/100] Colorization Loss: 0.0014\n",
      "Epoch [43/100] Inpainting Loss: 0.5189\n",
      "Epoch [44/100] Colorization Loss: 0.0014\n",
      "Epoch [44/100] Inpainting Loss: 0.5189\n",
      "Epoch [45/100] Colorization Loss: 0.0014\n",
      "Epoch [45/100] Inpainting Loss: 0.5188\n",
      "Epoch [46/100] Colorization Loss: 0.0013\n",
      "Epoch [46/100] Inpainting Loss: 0.5188\n",
      "Epoch [47/100] Colorization Loss: 0.0013\n",
      "Epoch [47/100] Inpainting Loss: 0.5187\n",
      "Epoch [48/100] Colorization Loss: 0.0013\n",
      "Epoch [48/100] Inpainting Loss: 0.5186\n",
      "Epoch [49/100] Colorization Loss: 0.0013\n",
      "Epoch [49/100] Inpainting Loss: 0.5186\n",
      "Epoch [50/100] Colorization Loss: 0.0013\n",
      "Epoch [50/100] Inpainting Loss: 0.5186\n",
      "Epoch [51/100] Colorization Loss: 0.0013\n",
      "Epoch [51/100] Inpainting Loss: 0.5185\n",
      "Epoch [52/100] Colorization Loss: 0.0013\n",
      "Epoch [52/100] Inpainting Loss: 0.5185\n",
      "Epoch [53/100] Colorization Loss: 0.0013\n",
      "Epoch [53/100] Inpainting Loss: 0.5184\n",
      "Epoch [54/100] Colorization Loss: 0.0013\n",
      "Epoch [54/100] Inpainting Loss: 0.5184\n",
      "Epoch [55/100] Colorization Loss: 0.0013\n",
      "Epoch [55/100] Inpainting Loss: 0.5184\n",
      "Epoch [56/100] Colorization Loss: 0.0013\n",
      "Epoch [56/100] Inpainting Loss: 0.5183\n",
      "Epoch [57/100] Colorization Loss: 0.0013\n",
      "Epoch [57/100] Inpainting Loss: 0.5182\n",
      "Epoch [58/100] Colorization Loss: 0.0013\n",
      "Epoch [58/100] Inpainting Loss: 0.5182\n",
      "Epoch [59/100] Colorization Loss: 0.0013\n",
      "Epoch [59/100] Inpainting Loss: 0.5181\n",
      "Epoch [60/100] Colorization Loss: 0.0013\n",
      "Epoch [60/100] Inpainting Loss: 0.5181\n",
      "Epoch [61/100] Colorization Loss: 0.0013\n",
      "Epoch [61/100] Inpainting Loss: 0.5180\n",
      "Epoch [62/100] Colorization Loss: 0.0013\n",
      "Epoch [62/100] Inpainting Loss: 0.5180\n",
      "Epoch [63/100] Colorization Loss: 0.0013\n",
      "Epoch [63/100] Inpainting Loss: 0.5180\n",
      "Epoch [64/100] Colorization Loss: 0.0013\n",
      "Epoch [64/100] Inpainting Loss: 0.5180\n",
      "Epoch [65/100] Colorization Loss: 0.0012\n",
      "Epoch [65/100] Inpainting Loss: 0.5179\n",
      "Epoch [66/100] Colorization Loss: 0.0012\n",
      "Epoch [66/100] Inpainting Loss: 0.5179\n",
      "Epoch [67/100] Colorization Loss: 0.0012\n",
      "Epoch [67/100] Inpainting Loss: 0.5178\n",
      "Epoch [68/100] Colorization Loss: 0.0012\n",
      "Epoch [68/100] Inpainting Loss: 0.5178\n",
      "Epoch [69/100] Colorization Loss: 0.0012\n",
      "Epoch [69/100] Inpainting Loss: 0.5178\n",
      "Epoch [70/100] Colorization Loss: 0.0012\n",
      "Epoch [70/100] Inpainting Loss: 0.5177\n",
      "Epoch [71/100] Colorization Loss: 0.0012\n",
      "Epoch [71/100] Inpainting Loss: 0.5177\n",
      "Epoch [72/100] Colorization Loss: 0.0012\n",
      "Epoch [72/100] Inpainting Loss: 0.5177\n",
      "Epoch [73/100] Colorization Loss: 0.0012\n",
      "Epoch [73/100] Inpainting Loss: 0.5176\n",
      "Epoch [74/100] Colorization Loss: 0.0012\n",
      "Epoch [74/100] Inpainting Loss: 0.5176\n",
      "Epoch [75/100] Colorization Loss: 0.0012\n",
      "Epoch [75/100] Inpainting Loss: 0.5176\n",
      "Epoch [76/100] Colorization Loss: 0.0012\n",
      "Epoch [76/100] Inpainting Loss: 0.5176\n",
      "Epoch [77/100] Colorization Loss: 0.0012\n",
      "Epoch [77/100] Inpainting Loss: 0.5175\n",
      "Epoch [78/100] Colorization Loss: 0.0012\n",
      "Epoch [78/100] Inpainting Loss: 0.5175\n",
      "Epoch [79/100] Colorization Loss: 0.0012\n",
      "Epoch [79/100] Inpainting Loss: 0.5175\n",
      "Epoch [80/100] Colorization Loss: 0.0012\n",
      "Epoch [80/100] Inpainting Loss: 0.5175\n",
      "Epoch [81/100] Colorization Loss: 0.0012\n",
      "Epoch [81/100] Inpainting Loss: 0.5174\n",
      "Epoch [82/100] Colorization Loss: 0.0012\n",
      "Epoch [82/100] Inpainting Loss: 0.5174\n",
      "Epoch [83/100] Colorization Loss: 0.0012\n",
      "Epoch [83/100] Inpainting Loss: 0.5174\n",
      "Epoch [84/100] Colorization Loss: 0.0012\n",
      "Epoch [84/100] Inpainting Loss: 0.5174\n",
      "Epoch [85/100] Colorization Loss: 0.0012\n",
      "Epoch [85/100] Inpainting Loss: 0.5173\n",
      "Epoch [86/100] Colorization Loss: 0.0012\n",
      "Epoch [86/100] Inpainting Loss: 0.5173\n",
      "Epoch [87/100] Colorization Loss: 0.0012\n",
      "Epoch [87/100] Inpainting Loss: 0.5173\n",
      "Epoch [88/100] Colorization Loss: 0.0012\n",
      "Epoch [88/100] Inpainting Loss: 0.5173\n",
      "Epoch [89/100] Colorization Loss: 0.0012\n",
      "Epoch [89/100] Inpainting Loss: 0.5173\n",
      "Epoch [90/100] Colorization Loss: 0.0012\n",
      "Epoch [90/100] Inpainting Loss: 0.5172\n",
      "Epoch [91/100] Colorization Loss: 0.0012\n",
      "Epoch [91/100] Inpainting Loss: 0.5172\n",
      "Epoch [92/100] Colorization Loss: 0.0012\n",
      "Epoch [92/100] Inpainting Loss: 0.5172\n",
      "Epoch [93/100] Colorization Loss: 0.0012\n",
      "Epoch [93/100] Inpainting Loss: 0.5172\n",
      "Epoch [94/100] Colorization Loss: 0.0012\n",
      "Epoch [94/100] Inpainting Loss: 0.5171\n",
      "Epoch [95/100] Colorization Loss: 0.0012\n",
      "Epoch [95/100] Inpainting Loss: 0.5171\n",
      "Epoch [96/100] Colorization Loss: 0.0012\n",
      "Epoch [96/100] Inpainting Loss: 0.5171\n",
      "Epoch [97/100] Colorization Loss: 0.0012\n",
      "Epoch [97/100] Inpainting Loss: 0.5171\n",
      "Epoch [98/100] Colorization Loss: 0.0012\n",
      "Epoch [98/100] Inpainting Loss: 0.5171\n",
      "Epoch [99/100] Colorization Loss: 0.0012\n",
      "Epoch [99/100] Inpainting Loss: 0.5171\n",
      "Epoch [100/100] Colorization Loss: 0.0012\n",
      "Epoch [100/100] Inpainting Loss: 0.5170\n"
     ]
    }
   ],
   "source": [
    "# Set mask size for inpainting\n",
    "mask_size = 8  # Adjust as needed\n",
    "\n",
    "# Training loop with combined pretext tasks per epoch\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    total_loss_colorization = 0.0\n",
    "    total_loss_inpainting = 0.0\n",
    "\n",
    "    colorization_iter = iter(colorization_loader)\n",
    "    inpainting_iter = iter(inpainting_loader)\n",
    "\n",
    "    # Alternate between colorization and inpainting batches within the epoch\n",
    "    for i in range(min(len(colorization_loader), len(inpainting_loader))):\n",
    "        # Colorization task\n",
    "        try:\n",
    "            L_channel, AB_channels = next(colorization_iter)\n",
    "            L_channel, AB_channels = L_channel.to(device), AB_channels.to(device)\n",
    "            L_channel_rgb = L_channel.repeat(1, 3, 1, 1)  # Convert grayscale to RGB input format\n",
    "\n",
    "            # Forward pass for colorization\n",
    "            predicted_AB = combined_model.forward_colorization(L_channel_rgb)\n",
    "            loss_colorization = colorization_criterion(predicted_AB, AB_channels)\n",
    "\n",
    "            # Backward pass and optimization for colorization\n",
    "            optimizer.zero_grad()\n",
    "            loss_colorization.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_colorization += loss_colorization.item()\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "        # Inpainting task\n",
    "        try:\n",
    "            images, _ = next(inpainting_iter)\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Apply masking with specified mask_size\n",
    "            masked_images, masks = mask_image(images, mask_size=mask_size)\n",
    "            masked_images, masks = masked_images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass for inpainting\n",
    "            reconstructed_images = combined_model.forward_inpainting(masked_images)\n",
    "            loss_inpainting = inpainting_criterion(reconstructed_images, images)\n",
    "\n",
    "            # Backward pass and optimization for inpainting\n",
    "            optimizer.zero_grad()\n",
    "            loss_inpainting.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_inpainting += loss_inpainting.item()\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    # Calculate average losses for the epoch\n",
    "    avg_loss_colorization = total_loss_colorization / len(colorization_loader)\n",
    "    avg_loss_inpainting = total_loss_inpainting / len(inpainting_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Colorization Loss: {avg_loss_colorization:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Inpainting Loss: {avg_loss_inpainting:.4f}')\n",
    "\n",
    "    # Early stopping check based on combined average loss\n",
    "    combined_avg_loss = (avg_loss_colorization + avg_loss_inpainting) / 2\n",
    "    early_stop(combined_avg_loss)\n",
    "    if early_stop.early_stop:\n",
    "        print(\"Early Stopping Triggered\")\n",
    "        break\n",
    "\n",
    "# Save the final combined model\n",
    "torch.save(combined_model.state_dict(), '/users/soh62/SSL/image_colorization_simultaneous/models2/combined_inpaint_colorization_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function to verify colorization predictions\n",
    "def visualize_colorization(L_channel, predicted_AB, ground_truth_AB):\n",
    "    batch_size = L_channel.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Convert model's output (predicted_AB) to RGB for each sample in the batch\n",
    "        colorized_image = lab_to_rgb(L_channel[i], predicted_AB[i])\n",
    "\n",
    "        # Convert the ground truth to RGB for each sample\n",
    "        ground_truth_rgb = lab_to_rgb(L_channel[i], ground_truth_AB[i])\n",
    "\n",
    "        # Display the colorized image and the ground truth (visualization code)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(colorized_image)\n",
    "        plt.title('Predicted Colorization')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(ground_truth_rgb)\n",
    "        plt.title('Ground Truth')\n",
    "\n",
    "        plt.show()\n",
    "        print(f\"Predicted AB min: {predicted_AB[i].min():.2f}, max: {predicted_AB[i].max():.2f}\")\n",
    "        print(f\"Ground Truth AB min: {ground_truth_AB[i].min():.2f}, max: {ground_truth_AB[i].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Epoch 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Linear(512, num_classes)  # Adjust input size based on backbone output\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        pooled_features = nn.AdaptiveAvgPool2d((1, 1))(features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        output = self.classifier(pooled_features)\n",
    "        return output\n",
    "\n",
    "# Initialize the classification model with the backbone from your trained combined model\n",
    "classification_model = ClassificationNet(combined_model.encoder, num_classes=10).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),  # RGB for classification\n",
    "])\n",
    "\n",
    "# Load STL10 train and test datasets\n",
    "stl10_train = STL10(root='../data', split='train', download=True, transform=classification_transform)\n",
    "stl10_test = STL10(root='../data', split='test', download=True, transform=classification_transform)\n",
    "\n",
    "# DataLoaders for training and testing\n",
    "train_loader = DataLoader(stl10_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(stl10_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.8470\n",
      "Epoch [2/50], Loss: 1.3864\n",
      "Epoch [3/50], Loss: 1.1259\n",
      "Epoch [4/50], Loss: 0.8858\n",
      "Epoch [5/50], Loss: 0.6475\n",
      "Epoch [6/50], Loss: 0.4143\n",
      "Epoch [7/50], Loss: 0.2225\n",
      "Epoch [8/50], Loss: 0.0978\n",
      "Epoch [9/50], Loss: 0.0557\n",
      "Epoch [10/50], Loss: 0.0200\n",
      "Epoch [11/50], Loss: 0.0192\n",
      "Epoch [12/50], Loss: 0.0393\n",
      "Epoch [13/50], Loss: 0.0212\n",
      "Epoch [14/50], Loss: 0.0151\n",
      "Epoch [15/50], Loss: 0.0359\n",
      "Epoch [16/50], Loss: 0.0529\n",
      "Epoch [17/50], Loss: 0.0124\n",
      "Epoch [18/50], Loss: 0.0047\n",
      "Epoch [19/50], Loss: 0.0034\n",
      "Epoch [20/50], Loss: 0.0028\n",
      "Epoch [21/50], Loss: 0.0019\n",
      "Epoch [22/50], Loss: 0.0015\n",
      "Epoch [23/50], Loss: 0.0050\n",
      "Epoch [24/50], Loss: 0.0550\n",
      "Epoch [25/50], Loss: 0.0282\n",
      "Epoch [26/50], Loss: 0.0388\n",
      "Epoch [27/50], Loss: 0.0109\n",
      "Epoch [28/50], Loss: 0.0083\n",
      "Epoch [29/50], Loss: 0.0315\n",
      "Epoch [30/50], Loss: 0.0674\n",
      "Epoch [31/50], Loss: 0.0411\n",
      "Epoch [32/50], Loss: 0.0065\n",
      "Epoch [33/50], Loss: 0.0034\n",
      "Epoch [34/50], Loss: 0.0139\n",
      "Epoch [35/50], Loss: 0.0274\n",
      "Epoch [36/50], Loss: 0.0238\n",
      "Epoch [37/50], Loss: 0.0058\n",
      "Epoch [38/50], Loss: 0.0060\n",
      "Epoch [39/50], Loss: 0.0224\n",
      "Epoch [40/50], Loss: 0.0162\n",
      "Epoch [41/50], Loss: 0.0187\n",
      "Epoch [42/50], Loss: 0.0031\n",
      "Epoch [43/50], Loss: 0.0031\n",
      "Epoch [44/50], Loss: 0.0145\n",
      "Epoch [45/50], Loss: 0.0033\n",
      "Epoch [46/50], Loss: 0.0011\n",
      "Epoch [47/50], Loss: 0.0043\n",
      "Epoch [48/50], Loss: 0.0399\n",
      "Epoch [49/50], Loss: 0.0097\n",
      "Epoch [50/50], Loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer for classification\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop for downstream classification task\n",
    "num_epochs = 50  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = classification_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Optionally save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(classification_model.state_dict(), f'models2/classification_model_weights2_epoch_{epoch+1}.pth')\n",
    "\n",
    "# Save the final classification model\n",
    "torch.save(classification_model.state_dict(), 'models2/classification_model_weights_final2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 62.16%\n",
      "Top-3 Accuracy: 87.97%\n",
      "Top-5 Accuracy: 95.24%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "classification_model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "top_3_correct = 0\n",
    "top_5_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Top-3 and Top-5 accuracy calculations\n",
    "        _, predicted_3 = torch.topk(outputs, k=3, dim=1)\n",
    "        correct_3 = predicted_3.eq(labels.view(-1, 1).expand_as(predicted_3))\n",
    "        top_3_correct += correct_3.any(dim=1).sum().item()\n",
    "\n",
    "        _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
    "        correct_5 = predicted_5.eq(labels.view(-1, 1).expand_as(predicted_5))\n",
    "        top_5_correct += correct_5.any(dim=1).sum().item()\n",
    "\n",
    "# Calculate and print accuracies\n",
    "accuracy = 100 * correct / total\n",
    "top_3_accuracy = 100 * top_3_correct / total\n",
    "top_5_accuracy = 100 * top_5_correct / total\n",
    "print(f'Top-1 Accuracy: {accuracy:.2f}%')\n",
    "print(f'Top-3 Accuracy: {top_3_accuracy:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_5_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Epoch 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Linear(512, num_classes)  # Adjust input size based on backbone output\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        pooled_features = nn.AdaptiveAvgPool2d((1, 1))(features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        output = self.classifier(pooled_features)\n",
    "        return output\n",
    "\n",
    "# Initialize the classification model with the backbone from your trained combined model\n",
    "classification_model = ClassificationNet(combined_model.encoder, num_classes=10).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),  # RGB for classification\n",
    "])\n",
    "\n",
    "# Load STL10 train and test datasets\n",
    "stl10_train = STL10(root='../data', split='train', download=True, transform=classification_transform)\n",
    "stl10_test = STL10(root='../data', split='test', download=True, transform=classification_transform)\n",
    "\n",
    "# DataLoaders for training and testing\n",
    "train_loader = DataLoader(stl10_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(stl10_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.4989\n",
      "Epoch [2/50], Loss: 0.0306\n",
      "Epoch [3/50], Loss: 0.0147\n",
      "Epoch [4/50], Loss: 0.0061\n",
      "Epoch [5/50], Loss: 0.0039\n",
      "Epoch [6/50], Loss: 0.0035\n",
      "Epoch [7/50], Loss: 0.0027\n",
      "Epoch [8/50], Loss: 0.0065\n",
      "Epoch [9/50], Loss: 0.0235\n",
      "Epoch [10/50], Loss: 0.0358\n",
      "Epoch [11/50], Loss: 0.0058\n",
      "Epoch [12/50], Loss: 0.0036\n",
      "Epoch [13/50], Loss: 0.0060\n",
      "Epoch [14/50], Loss: 0.0219\n",
      "Epoch [15/50], Loss: 0.0320\n",
      "Epoch [16/50], Loss: 0.0039\n",
      "Epoch [17/50], Loss: 0.0017\n",
      "Epoch [18/50], Loss: 0.0009\n",
      "Epoch [19/50], Loss: 0.0025\n",
      "Epoch [20/50], Loss: 0.0152\n",
      "Epoch [21/50], Loss: 0.0091\n",
      "Epoch [22/50], Loss: 0.0405\n",
      "Epoch [23/50], Loss: 0.0196\n",
      "Epoch [24/50], Loss: 0.0040\n",
      "Epoch [25/50], Loss: 0.0028\n",
      "Epoch [26/50], Loss: 0.0296\n",
      "Epoch [27/50], Loss: 0.0120\n",
      "Epoch [28/50], Loss: 0.0289\n",
      "Epoch [29/50], Loss: 0.0081\n",
      "Epoch [30/50], Loss: 0.0047\n",
      "Epoch [31/50], Loss: 0.0017\n",
      "Epoch [32/50], Loss: 0.0014\n",
      "Epoch [33/50], Loss: 0.0008\n",
      "Epoch [34/50], Loss: 0.0009\n",
      "Epoch [35/50], Loss: 0.0008\n",
      "Epoch [36/50], Loss: 0.0003\n",
      "Epoch [37/50], Loss: 0.0004\n",
      "Epoch [38/50], Loss: 0.0009\n",
      "Epoch [39/50], Loss: 0.0079\n",
      "Epoch [40/50], Loss: 0.0021\n",
      "Epoch [41/50], Loss: 0.0006\n",
      "Epoch [42/50], Loss: 0.0012\n",
      "Epoch [43/50], Loss: 0.0025\n",
      "Epoch [44/50], Loss: 0.0141\n",
      "Epoch [45/50], Loss: 0.0239\n",
      "Epoch [46/50], Loss: 0.0203\n",
      "Epoch [47/50], Loss: 0.0278\n",
      "Epoch [48/50], Loss: 0.0047\n",
      "Epoch [49/50], Loss: 0.0065\n",
      "Epoch [50/50], Loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer for classification\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop for downstream classification task\n",
    "num_epochs = 50  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = classification_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Optionally save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(classification_model.state_dict(), f'models2/classification_model_weights_final2_150_{epoch+1}.pth')\n",
    "\n",
    "# Save the final classification model\n",
    "torch.save(classification_model.state_dict(), 'models2/classification_model_weights_final2_150.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 61.00%\n",
      "Top-3 Accuracy: 86.15%\n",
      "Top-5 Accuracy: 93.91%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "classification_model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "top_3_correct = 0\n",
    "top_5_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Top-3 and Top-5 accuracy calculations\n",
    "        _, predicted_3 = torch.topk(outputs, k=3, dim=1)\n",
    "        correct_3 = predicted_3.eq(labels.view(-1, 1).expand_as(predicted_3))\n",
    "        top_3_correct += correct_3.any(dim=1).sum().item()\n",
    "\n",
    "        _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
    "        correct_5 = predicted_5.eq(labels.view(-1, 1).expand_as(predicted_5))\n",
    "        top_5_correct += correct_5.any(dim=1).sum().item()\n",
    "\n",
    "# Calculate and print accuracies\n",
    "accuracy = 100 * correct / total\n",
    "top_3_accuracy = 100 * top_3_correct / total\n",
    "top_5_accuracy = 100 * top_5_correct / total\n",
    "print(f'Top-1 Accuracy: {accuracy:.2f}%')\n",
    "print(f'Top-3 Accuracy: {top_3_accuracy:.2f}%')\n",
    "print(f'Top-5 Accuracy: {top_5_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
